{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2008ba4",
   "metadata": {
    "papermill": {
     "duration": 0.013174,
     "end_time": "2023-11-09T03:15:46.560482",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.547308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 사용 설명서\n",
    "\n",
    "## 0. requirements\n",
    "#### 0.1. 로컬 환경에서 package 설치법\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>```\n",
    "\n",
    "#### 0.2. kaggle api 설치법\n",
    "- https://www.kaggle.com/docs/api#getting-started-installation-&-authentication 참고\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle```\n",
    "- kaggle api token 다운로드 후 upload (kaggle.json)\n",
    "- ```!mkdir ~/.kaggle``` \n",
    "- ```!mv ~/kaggle.json ~/.kaggle/``` (kaggle.json 파일을 ~/.kaggle/ 로 이동)\n",
    "- ```!chmod 600 ~/.kaggle/kaggle.json``` (권한 설정)\n",
    "\n",
    "## 1. config 설정\n",
    "\n",
    "#### 1.1. init config\n",
    "- MODE: train, inference 중 선택 (train : 로컬 환경, inference : 캐글 환경)\n",
    "- KAGGLE_DATASET_NAME: 캐글 환경에서 inference 시 사용할 데이터셋 이름 \n",
    "  - 이 이름으로 캐글 데이터셋이 생성됩니다. (중복 불가)\n",
    "\n",
    "#### 1.2. train / inference config\n",
    "- model_directory: 모델 저장 경로\n",
    "- data_directory: 데이터 경로\n",
    "- train_mode: train 모드 여부\n",
    "- infer_mode: inference 모드 여부\n",
    "\n",
    "#### 1.3. model config\n",
    "- model_name: 사용할 모델 이름\n",
    "    - 실제 아래 models_config에 있는 모델 이름과 동일해야 합니다 (아래중에서 선택하는것임).(:list)\n",
    "- target: target column 이름\n",
    "- split_method: 데이터 분리 방식\n",
    "  - time_series: 시계열 데이터 분리\n",
    "  - rolling: 롤링 윈도우 방식 데이터 분리\n",
    "  - blocking: 블록 방식 데이터 분리\n",
    "  - holdout: holdout 방식 데이터 분리\n",
    "- n_splits: 데이터 분리 개수 (1 ~)\n",
    "- correct: 데이터 분리 시 날짜 boundary를 맞출지 여부 (True / False)\n",
    "- initial_fold_size_ratio: 초기 fold size 비율 (0 ~ 1)\n",
    "- train_test_ratio: train, test 비율 (0 ~ 1)\n",
    "- ~train_start: 학습 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~train_end: 학습 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_start: 검증 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_end: 검증 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- optuna_random_state: optuna random state\n",
    "                                - \n",
    "#### 1.4. model heyperparameter config\n",
    "- models_config: 모델 하이퍼파라미터 설정\n",
    "    - model: 모델 클래스\n",
    "    - params: 모델 하이퍼파라미터들\n",
    "        - ... : 모델 하이퍼파라미터\n",
    "\n",
    "## 2. Global Method\n",
    "- reduce_mem_usage: 메모리 사용량 줄이는 함수\n",
    "- compute_triplet_imbalance: triplet imbalance 계산 함수\n",
    "- calculate_triplet_imbalance_numba: triplet imbalance 계산 함수\n",
    "- print_log: 함수 실행 전후에 원하는 코드를 실행해주는 decorator 함수입니다.\n",
    "- zero_sum: zero sum 함수\n",
    "\n",
    "## 3. Pre Code\n",
    "- DataPreprocessor: 데이터 전처리 클래스\n",
    "- FeatureEngineer: 피쳐 엔지니어링 클래스\n",
    "- Splitter: 데이터 분리 클래스\n",
    "- Model: 모델 클래스\n",
    "- Trainer: 학습 클래스\n",
    "\n",
    "## 4. Main Code\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294dbecd",
   "metadata": {
    "papermill": {
     "duration": 0.013176,
     "end_time": "2023-11-09T03:15:46.586959",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.573783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf15808",
   "metadata": {
    "papermill": {
     "duration": 0.013293,
     "end_time": "2023-11-09T03:15:46.613651",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.600358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099e1685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276368Z",
     "start_time": "2023-11-18T04:44:53.963992Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T07:39:27.502885Z",
     "iopub.status.busy": "2023-12-03T07:39:27.502475Z",
     "iopub.status.idle": "2023-12-03T07:39:27.510799Z",
     "shell.execute_reply": "2023-12-03T07:39:27.509858Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021933,
     "end_time": "2023-11-09T03:15:46.649226",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.627293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e3578",
   "metadata": {
    "papermill": {
     "duration": 0.014012,
     "end_time": "2023-11-09T03:15:46.677485",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.663473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 0.2. kaggle api 설치법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d102fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276461Z",
     "start_time": "2023-11-18T04:44:53.998348Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T07:39:27.516088Z",
     "iopub.status.busy": "2023-12-03T07:39:27.515618Z",
     "iopub.status.idle": "2023-12-03T07:39:27.527824Z",
     "shell.execute_reply": "2023-12-03T07:39:27.526982Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021246,
     "end_time": "2023-11-09T03:15:46.712389",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.691143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c23ba22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276579Z",
     "start_time": "2023-11-18T04:44:54.001642Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T07:39:27.532843Z",
     "iopub.status.busy": "2023-12-03T07:39:27.532387Z",
     "iopub.status.idle": "2023-12-03T07:39:27.536879Z",
     "shell.execute_reply": "2023-12-03T07:39:27.536046Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020857,
     "end_time": "2023-11-09T03:15:46.747089",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.726232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b345d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276667Z",
     "start_time": "2023-11-18T04:44:54.001730Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T07:39:27.541894Z",
     "iopub.status.busy": "2023-12-03T07:39:27.541324Z",
     "iopub.status.idle": "2023-12-03T07:39:27.545683Z",
     "shell.execute_reply": "2023-12-03T07:39:27.544867Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02085,
     "end_time": "2023-11-09T03:15:46.782033",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.761183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mv ~/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b15cb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276757Z",
     "start_time": "2023-11-18T04:44:54.001870Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T07:39:27.550215Z",
     "iopub.status.busy": "2023-12-03T07:39:27.549814Z",
     "iopub.status.idle": "2023-12-03T07:39:27.554628Z",
     "shell.execute_reply": "2023-12-03T07:39:27.553810Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021282,
     "end_time": "2023-11-09T03:15:46.817302",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.796020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f24c62",
   "metadata": {
    "papermill": {
     "duration": 0.013582,
     "end_time": "2023-11-09T03:15:46.844948",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.831366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. config 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70dd73",
   "metadata": {
    "papermill": {
     "duration": 0.013741,
     "end_time": "2023-11-09T03:15:46.924523",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.910782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1. init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9db272d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276841Z",
     "start_time": "2023-11-18T04:44:54.045651Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026122,
     "end_time": "2023-11-09T03:15:46.964537",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.938415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference, both\n",
    "KAGGLE_DATASET_NAME = \"model-nn-version-yongmin-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0199e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.081563Z",
     "start_time": "2023-11-18T04:44:54.045860Z"
    },
    "papermill": {
     "duration": 5.644978,
     "end_time": "2023-11-09T03:16:25.965358",
     "exception": false,
     "start_time": "2023-11-09T03:16:20.320380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 11:39:19.087927: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-08 11:39:19.106815: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-08 11:39:19.106832: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-08 11:39:19.106843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-08 11:39:19.110469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import functools\n",
    "import time\n",
    "from numba import njit, prange\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed4ef3",
   "metadata": {
    "papermill": {
     "duration": 0.014983,
     "end_time": "2023-11-09T03:16:25.995747",
     "exception": false,
     "start_time": "2023-11-09T03:16:25.980764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.2. train / inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd127d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.084927Z",
     "start_time": "2023-11-18T04:44:55.081831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "EPS = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7244d611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.174297Z",
     "start_time": "2023-11-18T04:44:55.082342Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025998,
     "end_time": "2023-11-09T03:16:26.037324",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.011326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d61a0b",
   "metadata": {
    "papermill": {
     "duration": 0.01504,
     "end_time": "2023-11-09T03:16:26.067759",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.052719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.3. model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809caa6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.178131Z",
     "start_time": "2023-11-18T04:44:55.160976Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026838,
     "end_time": "2023-11-09T03:16:26.109734",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.082896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"final_mode\": False,  # True : using final model, False : not using final model\n",
    "    \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "    'target': 'target',\n",
    "\n",
    "    'split_method': 'rolling',  # time_series, rolling, blocking, holdout\n",
    "    'n_splits': 3,  # number of splits\n",
    "    'correct': True,  # correct boundary\n",
    "    'gap': 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "\n",
    "    'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "    'train_test_ratio': 0.9,  # train, test ratio\n",
    "\n",
    "    'optuna_random_state': 42,\n",
    "}\n",
    "config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "    \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "    \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd8554",
   "metadata": {
    "papermill": {
     "duration": 0.015075,
     "end_time": "2023-11-09T03:16:26.139996",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.124921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.4. model heyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85133e2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.582Z",
     "start_time": "2023-11-18T04:44:55.161493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      " 98%|███████████████████████████████████████▎| 197M/201M [00:06<00:00, 30.7MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:06<00:00, 31.4MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2db0d",
   "metadata": {
    "papermill": {
     "duration": 0.015155,
     "end_time": "2023-11-09T03:16:26.253174",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.238019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Global Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f2d234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.598960Z",
     "start_time": "2023-11-18T04:45:03.585612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030413,
     "end_time": "2023-11-09T03:16:26.299001",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.268588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643c526f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644285Z",
     "start_time": "2023-11-18T04:45:03.620103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035964,
     "end_time": "2023-11-09T03:16:26.484162",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.448198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02025bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644943Z",
     "start_time": "2023-11-18T04:45:03.620276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0316,
     "end_time": "2023-11-09T03:16:26.537507",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.505907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0184e",
   "metadata": {
    "papermill": {
     "duration": 0.017946,
     "end_time": "2023-11-09T03:16:26.571351",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.553405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 각 클래스의 method는 각자 필요에 따라 추가 해서 사용하면 됩니다. 이때 class의 주석에 method를 추가하고, method의 주석에는 method의 역할을 간단하게 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23edb5",
   "metadata": {
    "papermill": {
     "duration": 0.019169,
     "end_time": "2023-11-09T03:16:26.609856",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.590687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Pre Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318c17",
   "metadata": {
    "papermill": {
     "duration": 0.019027,
     "end_time": "2023-11-09T03:16:26.648726",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.629699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214524d",
   "metadata": {
    "papermill": {
     "duration": 0.018425,
     "end_time": "2023-11-09T03:16:26.733619",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.715194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0b61fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.863864Z",
     "start_time": "2023-11-18T04:45:03.863395Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a366a19-e606-4f36-9c56-1dc94bd8de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window_size=7):\n",
    "    price_diff = data['wap'].diff()\n",
    "    gain = price_diff.where(price_diff > 0, 0)\n",
    "    loss = -price_diff.where(price_diff < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window_size).mean()\n",
    "    avg_loss = loss.rolling(window=window_size).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6319782-253f-4771-ae4b-9438a6990595",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5799b34f-1fd2-45c6-923c-fb68f62ce2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 3131/3131 [00:05<00:00, 600.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "all_stock_data = {}\n",
    "\n",
    "for s in tqdm(glob.glob(\"./data/alpha/*.csv\") if MODE == \"train\" else glob.glob(\n",
    "        \"/kaggle/input/nasdaq-stocks-historical-data/alpha/*.csv\"), desc=\"Processing files\"):\n",
    "    stock_df = pd.read_csv(s, dtype={\"ticker\": str})\n",
    "    stock_df.query(\"Date >= '2021-08-05' and Date <= '2023-07-06'\", inplace=True)\n",
    "    if len(stock_df) > 180:\n",
    "        all_stock_data[s[13:-15]] = (stock_df, len(stock_df))\n",
    "\n",
    "reversed_stock_list = [\n",
    "        'MNST', 'WING', 'AXON', 'HON', 'MAR', 'OKTA', 'POOL', 'LRCX', 'YOTA', 'PFG',\n",
    "        'NDAQ', 'COIN', 'AMGN', 'TER', 'ADBE', 'ABNB', 'ZBRA', 'KLAC', 'ZI', 'ALNY',\n",
    "        'ULTA', 'SSNC', 'ON', 'SWKS', 'AKAM', 'ASML', 'PPBI', 'QRVO', 'FANG', 'ORLY',\n",
    "        'LNT', 'AGRX', 'NTAP', 'CROX', 'REGN', 'ROST', 'DLTR', 'ADP', 'EMCG', 'CTAS',\n",
    "        'CZR', 'NVDA', 'SAIA', 'JKHY', 'FOSLL', 'MSFT', 'TECH', 'TXRH', 'WDAY', 'FITB',\n",
    "        'MTCH', 'ROKU', 'CINF', 'EBAY', 'SNPS', 'FAST', 'ETSY', 'IDXX', 'INTU', 'ZG',\n",
    "        'CRWD', 'LYFT', 'RGEN', 'LKQ', 'MKTX', 'EXC', 'LBRDK', 'MRNA', 'PAYX', 'SOFI',\n",
    "        'BYND', 'EQIX', 'ADI', 'GEN', 'ALGN', 'CDNS', 'HAS', 'VRTX', 'HOOD', 'WBD',\n",
    "        'TXG', 'SGEN', 'OPEN', 'INTC', 'GOOG', 'CAR', 'UPST', 'LSCC', 'NFLX', 'ENTG',\n",
    "        'FFIV', 'DOCU', 'MSTR', 'ZION', 'PCTY', 'AMD', 'MRVL', 'NBIX', 'JBLU', 'PARA',\n",
    "        'MQ', 'FCNCA', 'TEAM', 'ZS', 'WBA', 'MDLZ', 'TRMB', 'PODD', 'SEDG', 'CSX',\n",
    "        'TMUS', 'SPWR', 'AAPL', 'LULU', 'LPLA', 'ILMN', 'CDW', 'GDS', 'MELI', 'MASI',\n",
    "        'FOXA', 'KDP', 'AAL', 'GILD', 'ASO', 'UTHR', 'MU', 'MDB', 'WDC', 'CFLT',\n",
    "        'SBUX', 'INCY', 'TSCO', 'ISRG', 'VTRS', 'DKNG', 'LITE', 'TTWO', 'SMCI', 'EXPE',\n",
    "        'VRTS', 'AMAT', 'AVGO', 'TLRY', 'PCAR', 'CG', 'MIDD', 'APA', 'LNT', 'VRSK',\n",
    "        'PANW', 'CSCO', 'SBAC', 'HTZ', 'DBX', 'CHKEW', 'LCID', 'ADSK', 'APLS', 'STLD',\n",
    "        'PEP', 'PTON', 'ENPH', 'COST', 'CPRT', 'HST', 'KHC', 'CHRW', 'AMZN', 'ANSS',\n",
    "        'HOLX', 'TROW', 'APP', 'FIVE', 'AFRM', 'GOOGL', 'FTNT', 'SWAV', 'ZM', 'META',\n",
    "        'GH', 'JBHT', 'UAL', 'MCHP', 'DDOG', 'ODFL', 'CTSH', 'EA', 'RUN', 'CSGP',\n",
    "        'DXCM', 'TSLA', 'PTC', 'PYPL', 'PENN', 'XEL', 'XRAY', 'SPLK', 'CMCSA', 'BKR'\n",
    "]\n",
    "\n",
    "stock_list_df = pd.read_csv('./data/nasdaq-screener/nasdaq_screener_1701158836955.csv') if MODE == \"train\" else pd.read_csv(\n",
    "    '/kaggle/input/nasdaq-screener/nasdaq_screener_1701158836955.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43c4bd3-c2d0-41a6-9d73-68542776537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_stock_info(df, data, column_name):  # column_name = \"Market Cap\", \"Sector\", \"Industry\"\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    if column_name != \"Market Cap\":\n",
    "        stock_list_df[column_name] = le.fit_transform(stock_list_df[column_name])\n",
    "\n",
    "    df[f'{column_name}'] = -1\n",
    "\n",
    "    for idx, ticker in enumerate(reversed_stock_list):\n",
    "        stock_id_indices = data[data['stock_id'] == idx].index\n",
    "        if ticker in stock_list_df[\"Symbol\"].values:\n",
    "            value = stock_list_df[stock_list_df[\"Symbol\"] == ticker][column_name].iloc[0]\n",
    "            df.loc[stock_id_indices, f'{column_name}'] = value\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b428ec7e-428e-4678-9c73-5a3934a39886",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "_weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c63a599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.937718Z",
     "start_time": "2023-11-18T04:45:03.863730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_0(self, *args, version_name=\"feature_version_yongmin_0\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df['dow'] = self.data[\"date_id\"] % 5\n",
    "        df['seconds'] = self.data['seconds_in_bucket'] % 60\n",
    "        df['minute'] = self.data['seconds_in_bucket'] // 60\n",
    "    \n",
    "        df[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "        df['cum_wap'] = self.data.groupby(['stock_id'])['wap'].cumprod()\n",
    "    \n",
    "        for i in [1, 6]:\n",
    "            df[f'pct_change_{i}'] = self.data.groupby(['stock_id', 'seconds_in_bucket'])['wap'].pct_change(i).fillna(0)\n",
    "\n",
    "            f = lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "            df[f'polarize_pct_{i}'] = df[f'pct_change_{i}'].apply(f)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_1(self, *args, version_name=\"feature_version_yongmin_1\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        \n",
    "        window_size = 6\n",
    "        short_window = 1\n",
    "        long_window = 6\n",
    "\n",
    "        df['vol_std'] = self.data.groupby(['stock_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['rolling_vol_di'] = self.data.groupby(['date_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['std_st'] = self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['wap_pctch'] = self.data.groupby(['stock_id','date_id'])['wap'].pct_change().values*100\n",
    "        df['short_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=short_window, adjust=False).mean().values\n",
    "        df['long_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=long_window, adjust=False).mean().values\n",
    "        wap_mean = self.data['wap'].mean()\n",
    "        df['wap_vs_market'] = self.data['wap'] - self.data.groupby(['stock_id'])['wap'].transform('mean')\n",
    "        df['macd'] = df['short_ema'] - df['long_ema']\n",
    "        \n",
    "        # Bollinger Bands calculation within each stock, date, and time\n",
    "        df['bollinger_upper'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values + 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['bollinger_lower'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values - 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        # RSI calculation within each stock, date, and time\n",
    "        df['rsi'] = self.data.groupby(['stock_id']).apply(calculate_rsi).values\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_2(self, *args, version_name=\"feature_version_yongmin_2\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        self.data[\"stock_weights\"] = self.data[\"stock_id\"].map(_weights)\n",
    "        self.data[\"weighted_wap\"] = self.data[\"stock_weights\"] * self.data[\"wap\"]\n",
    "        df['wap_momentum'] = self.data.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "\n",
    "        df[\"imbalance_momentum\"] = self.data.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / self.data['matched_size']\n",
    "        self.data[\"price_spread\"] = self.data[\"ask_price\"] - self.data[\"bid_price\"]\n",
    "        \n",
    "        self.data[\"mid_price\"] = self.data.eval(\"(ask_price + bid_price) / 2\")\n",
    "        self.data[\"liquidity_imbalance\"] = self.data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n",
    "        df[\"matched_imbalance\"] = self.data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n",
    "        df[\"size_imbalance\"] = self.data.eval(f\"bid_size / ask_size+{EPS}\")\n",
    "        \n",
    "        df[\"spread_intensity\"] = self.data.groupby(['stock_id'])['price_spread'].diff()\n",
    "        df['price_pressure'] = self.data['imbalance_size'] * (self.data['ask_price'] - self.data['bid_price'])\n",
    "        df['market_urgency'] = self.data['price_spread'] * self.data['liquidity_imbalance']\n",
    "        df['depth_pressure'] = (self.data['ask_size'] - self.data['bid_size']) * (self.data['far_price'] - self.data['near_price'])\n",
    "        \n",
    "        df['spread_depth_ratio'] = (self.data['ask_price'] - self.data['bid_price']) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        df['mid_price_movement'] = self.data['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "        \n",
    "        df['micro_price'] = ((self.data['bid_price'] * self.data['ask_size']) + (self.data['ask_price'] * self.data['bid_size'])) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        df['relative_spread'] = (self.data['ask_price'] - self.data['bid_price']) / self.data['wap']\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_1(self, *args, version_name=\"feature_version_alvin_1\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = self.data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n",
    "\n",
    "        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "            triplet_feature = calculate_triplet_imbalance_numba(c, self.data)\n",
    "            df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_market_cap(self, *args, version_name=\"feature_market_cap\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Market Cap\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_sector(self, *args, version_name=\"feature_sector\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Sector\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_industry(self, *args, version_name=\"feature_industry\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Industry\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        final_data = concat_df\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb691e6",
   "metadata": {
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-11-09T03:16:26.835377",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.819592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f049e982-8438-430e-99d1-c0ecbab018c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \"\"\"\n",
    "    데이터 분리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        데이터 분리 방식\n",
    "    n_splits : int\n",
    "        데이터 분리 개수\n",
    "    correct : bool\n",
    "        데이터 분리 시 boundary를 맞출지 여부\n",
    "    initial_fold_size_ratio : float\n",
    "        초기 fold size 비율\n",
    "    train_test_ratio : float\n",
    "        train, test 비율\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    split()\n",
    "        데이터 분리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n",
    "                 overlap=True, train_start=0,\n",
    "                 train_end=390, valid_start=391, valid_end=480):\n",
    "        self.method = method\n",
    "        self.n_splits = n_splits\n",
    "        self.correct = correct\n",
    "        self.initial_fold_size_ratio = initial_fold_size_ratio\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "\n",
    "        self.gap = gap\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # only for holdout method\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.valid_start = valid_start\n",
    "        self.valid_end = valid_end\n",
    "\n",
    "        self.target = config[\"target\"]\n",
    "\n",
    "        self.boundaries = []\n",
    "\n",
    "    def split(self, data):\n",
    "        self.data = data #reduce_mem_usage(data)\n",
    "        self.all_dates = self.data['date_id_copy'].unique()\n",
    "        if self.method == \"time_series\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Time series split method only works with n_splits > 1\")\n",
    "            return self._time_series_split(data)\n",
    "        elif self.method == \"rolling\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"blocking\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n",
    "            self.initial_fold_size_ratio = 1.0 / self.n_splits\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"holdout\":\n",
    "            if self.n_splits != 1:\n",
    "                raise ValueError(\"Holdout method only works with n_splits=1\")\n",
    "            return self._holdout_split(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "    def _correct_boundary(self, data, idx, direction=\"forward\"):\n",
    "        # Correct the boundary based on date_id_copy\n",
    "        original_idx = idx\n",
    "        if idx == 0 or idx == len(data) - 1:\n",
    "            return idx\n",
    "        if direction == \"forward\":\n",
    "            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx += 1\n",
    "        elif direction == \"backward\":\n",
    "            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx -= 1\n",
    "            idx += 1  # adjust to include the boundary\n",
    "        return idx\n",
    "\n",
    "    def _time_series_split(self, data):\n",
    "        n = len(data)\n",
    "        initial_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n",
    "        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n",
    "            train_size = fold_size - initial_test_size\n",
    "\n",
    "            if self.correct:\n",
    "                train_size = self._correct_boundary(data, train_size, \"forward\")\n",
    "                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n",
    "            else:\n",
    "                end_of_test = train_size + initial_test_size\n",
    "\n",
    "            train_slice = data.iloc[:train_size]\n",
    "            test_slice = data.iloc[train_size:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "\n",
    "    def _rolling_split(self, data):\n",
    "        n = len(data)\n",
    "        total_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n",
    "        gap_size = int(total_fold_size * self.gap)\n",
    "        train_size = total_fold_size - test_size\n",
    "        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n",
    "\n",
    "        end_of_test = n - 1\n",
    "        start_of_test = end_of_test - test_size\n",
    "        end_of_train = start_of_test - gap_size\n",
    "        start_of_train = end_of_train - train_size\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            if self.correct:\n",
    "                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n",
    "                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n",
    "                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n",
    "                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n",
    "\n",
    "            train_slice = data[start_of_train:end_of_train]\n",
    "            test_slice = data[start_of_test:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[0],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "            start_of_train = max(start_of_train - rolling_increment, 0)\n",
    "            end_of_train -= rolling_increment\n",
    "            start_of_test -= rolling_increment\n",
    "            end_of_test -= rolling_increment\n",
    "\n",
    "    def _holdout_split(self, data):\n",
    "        # train_start ~ train_end : 학습 데이터 기간\n",
    "        # valid_start ~ valid_end : 검증 데이터 기간\n",
    "        # 학습 및 검증 데이터 분리\n",
    "        threshold = int(data['date_id_copy'].nunique() * self.train_test_ratio)\n",
    "        self.train_start, self.train_end = 0, data['date_id_copy'].unique()[threshold]\n",
    "        self.valid_start, self.valid_end = self.train_end + self.gap, data['date_id_copy'].unique()[-1]\n",
    "        \n",
    "        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n",
    "        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n",
    "\n",
    "        train_slice = data[train_mask]\n",
    "        valid_slice = data[valid_mask]\n",
    "\n",
    "        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_train = train_slice[self.target]\n",
    "        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_valid = valid_slice[self.target]\n",
    "\n",
    "        self.boundaries.append((\n",
    "            train_slice['date_id_copy'].iloc[0],\n",
    "            train_slice['date_id_copy'].iloc[-1],\n",
    "            valid_slice['date_id_copy'].iloc[0],\n",
    "            valid_slice['date_id_copy'].iloc[-1]\n",
    "        ))\n",
    "        yield X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    def visualize_splits(self):\n",
    "        print(\"Visualizing Train/Test Split Boundaries\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n",
    "            train_width = train_end - train_start + 1\n",
    "            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n",
    "            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n",
    "                     color='black', fontsize=8)\n",
    "\n",
    "            test_width = test_end - test_start + 1\n",
    "            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n",
    "            if test_width > 0:\n",
    "                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n",
    "                         color='black', fontsize=8)\n",
    "\n",
    "        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n",
    "        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n",
    "        plt.xlabel(\"date_id_copy\")\n",
    "        plt.title(\"Train/Test Split Boundaries\")\n",
    "        plt.grid(axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a91ec6",
   "metadata": {
    "papermill": {
     "duration": 0.017633,
     "end_time": "2023-11-09T03:16:26.985144",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.967511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bb36e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(num_continuous_features, num_categorical_features, embedding_dims, num_labels, hidden_units, dropout_rates, learning_rate,l2_strength=0.01):\n",
    "    \n",
    "    # Numerical variables input\n",
    "    input_continuous = tf.keras.layers.Input(shape=(num_continuous_features,))\n",
    "    \n",
    "    # Categorical variables input\n",
    "    input_categorical = [tf.keras.layers.Input(shape=(1,)) \n",
    "                         for _ in range(len(num_categorical_features))]\n",
    "    \n",
    "    # Embedding layer for categorical variables\n",
    "    embeddings = [tf.keras.layers.Embedding(input_dim=num_categorical_features[i] + 1, \n",
    "                                            output_dim=embedding_dims[i], \n",
    "                                            embeddings_initializer='he_normal')(input_cat) \n",
    "                  for i, input_cat in enumerate(input_categorical)]\n",
    "    flat_embeddings = [tf.keras.layers.Flatten()(embed) for embed in embeddings]\n",
    "    \n",
    "    # concat numerical and categorical\n",
    "    concat_input = tf.keras.layers.concatenate([input_continuous] + flat_embeddings)\n",
    "    \n",
    "    # MLP\n",
    "    x = tf.keras.layers.BatchNormalization()(concat_input)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    \n",
    "    for i in range(len(hidden_units)): \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)\n",
    "        x = tf.keras.layers.Dense(hidden_units[i], kernel_initializer='he_normal')(x)\n",
    "        # x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        \n",
    "    #No activation\n",
    "    out = tf.keras.layers.Dense(num_labels, kernel_initializer='he_normal')(x) \n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_continuous] + input_categorical, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c19ac",
   "metadata": {
    "papermill": {
     "duration": 0.016047,
     "end_time": "2023-11-09T03:16:27.179270",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.163223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Main\n",
    "## import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2de57-7bd7-42c6-b89c-2c43d0fc16c1",
   "metadata": {},
   "source": [
    "quantile transformer works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c36a1a0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:04.014440Z",
     "start_time": "2023-11-18T04:45:03.999301Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 피쳐 엔지니어링 할 함수에 args가 들어간다면 dependencies에 추가\n",
    "dependencies = {\n",
    "     # \"feature_version_yongmin_1\": [\"feature_version_yongmin_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2990a80b-036e-4790-9135-ea536406fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileTF:\n",
    "    def __init__(self):\n",
    "        self.scaler = QuantileTransformer(output_distribution='normal')\n",
    "        self.pipe = None\n",
    "\n",
    "    def initialize_pipeline(self, df, usecols, passcols):\n",
    "        columnTF = ColumnTransformer(\n",
    "                                        transformers = [\n",
    "                                            (\"scaler\" , self.scaler, usecols),\n",
    "                                            (\"pass\", 'passthrough', passcols)\n",
    "                                        ]\n",
    "                                    )\n",
    "\n",
    "        self.pipe = Pipeline([\n",
    "                                (\"scaler\", columnTF)\n",
    "                            ])\n",
    "\n",
    "    def fit(self, df, usecols, passcols):\n",
    "        self.initialize_pipeline(df, usecols, passcols)\n",
    "        self.pipe.fit(df)\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return self.pipe.transform(df)\n",
    "\n",
    "    def fit_transform(self, df, usecols, passcols):\n",
    "        self.initialize_pipeline(df, usecols, passcols)\n",
    "        return self.pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "212972c6-eb15-41f6-9118-804a04acc383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.80 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_0, Elapsed time: 2.54 seconds, shape((5237980, 9))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_1, Elapsed time: 11.50 seconds, shape((5237980, 11))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_2, Elapsed time: 1.29 seconds, shape((5237980, 12))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_alvin_1, Elapsed time: 1.25 seconds, shape((5237980, 23))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_market_cap, Elapsed time: 1.70 seconds, shape((5237980, 1))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_sector, Elapsed time: 1.68 seconds, shape((5237980, 1))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_industry, Elapsed time: 1.67 seconds, shape((5237980, 1))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 21.62 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 0.39 seconds, shape((5237980, 78))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 22.90 seconds, shape((5237980, 81))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "(5138980, 77) (5138980,) (55000, 77) (55000,)\n",
      "X_train_numerical shape: (5138980, 74)\n",
      "X_train_categorical shape: (5138980, 3)\n",
      "Y_train shape: (5138980,)\n",
      "\n",
      "\n",
      "X_test_numerical shape: (55000, 74)\n",
      "X_test_categorical shape: (55000, 3)\n",
      "Y_test shape: (55000,)\n",
      "\n",
      "Fitting Model - Holdout\n",
      "Epoch 1/15\n",
      "25695/25695 [==============================] - 134s 5ms/step - loss: 6.3052 - mae: 6.3052 - val_loss: 5.2204 - val_mae: 5.2204 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "25695/25695 [==============================] - 130s 5ms/step - loss: 6.2865 - mae: 6.2865 - val_loss: 5.2206 - val_mae: 5.2206 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "25695/25695 [==============================] - 131s 5ms/step - loss: 6.2791 - mae: 6.2791 - val_loss: 5.2172 - val_mae: 5.2172 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "25695/25695 [==============================] - 104s 4ms/step - loss: 6.2742 - mae: 6.2742 - val_loss: 5.2218 - val_mae: 5.2218 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "25695/25695 [==============================] - 129s 5ms/step - loss: 6.2696 - mae: 6.2696 - val_loss: 5.2216 - val_mae: 5.2216 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "25695/25695 [==============================] - 119s 5ms/step - loss: 6.2660 - mae: 6.2660 - val_loss: 5.2291 - val_mae: 5.2291 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "25695/25695 [==============================] - 105s 4ms/step - loss: 6.2590 - mae: 6.2590 - val_loss: 5.2286 - val_mae: 5.2286 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "25695/25695 [==============================] - 116s 5ms/step - loss: 6.2557 - mae: 6.2557 - val_loss: 5.2272 - val_mae: 5.2272 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "25695/25695 [==============================] - 126s 5ms/step - loss: 6.2536 - mae: 6.2536 - val_loss: 5.2254 - val_mae: 5.2254 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "25695/25695 [==============================] - 112s 4ms/step - loss: 6.2494 - mae: 6.2494 - val_loss: 5.2312 - val_mae: 5.2312 - lr: 2.5000e-04\n",
      "275/275 [==============================] - 0s 1ms/step\n",
      "Train NN Score: 5.217222\n"
     ]
    }
   ],
   "source": [
    "if config[\"train_mode\"]:\n",
    "    \n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "\n",
    "    # 데이터 전처리\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # 사용할 피쳐 엔지니어링 함수 선택\n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', 'feature_version_yongmin_2',\n",
    "                                                             'feature_version_alvin_1', 'feature_market_cap', 'feature_sector', 'feature_industry'],\n",
    "                                       dependencies=dependencies)\n",
    "    \n",
    "    feature_engineer.generate_global_features(df)\n",
    "    \n",
    "    df = feature_engineer.transform()  # 맨 처음에는 save=True 돌렸으면, 다음부턴 transform(load=True)로 바꾸면된 \n",
    "\n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "\n",
    "    df = df.drop(['row_id', 'time_id'], axis=1)\n",
    "    df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "\n",
    "    batch_size = 200\n",
    "    epochs = 15\n",
    "    hidden_units = [128, 128, 32] # 3 layer\n",
    "    dropout_rates = [0, 0.1, 0.1, 0.1]\n",
    "    learning_rate = 1e-3\n",
    "    embedding_dims = [50, 10, 10]\n",
    "    MODEL_NAME = \"my_nn_model_15epochV6_0.h5\"\n",
    "    \n",
    "    # set scaler\n",
    "    ckp_path = os.path.join(config['model_dir'], MODEL_NAME)\n",
    "    if not os.path.exists(config['model_dir']):\n",
    "        os.mkdir(config['model_dir'])\n",
    "    \n",
    "    rlr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=3, verbose=0, min_delta=1e-5, mode='min')\n",
    "    ckp = ModelCheckpoint(ckp_path, monitor='val_mae', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    es = EarlyStopping(monitor='val_mae', min_delta=1e-4, patience=7, mode='min', restore_best_weights=True, verbose=0)\n",
    "\n",
    "    model_checkpoint = [rlr, ckp, es]\n",
    "\n",
    "    splitter = Splitter(method='holdout', n_splits=1, correct=True, train_test_ratio=0.98, gap=5)\n",
    "    for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(df)):\n",
    "        if 'date_id_copy' in X_train.columns:\n",
    "            X_train = X_train.drop(['date_id_copy'], axis=1)\n",
    "            X_test  = X_test.drop(['date_id_copy'], axis=1)\n",
    "            \n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        scaler = QuantileTF()\n",
    "\n",
    "        categorical = [\"stock_id\", \"Sector\", \"Industry\"]\n",
    "        numerical = list(set(X_train.columns) - set(categorical) - set(['target', 'date_id_copy']))\n",
    "        num_categorical = [len(X_train[col].unique()) for col in categorical]\n",
    "    \n",
    "        unselect_columns = ['dow', 'seconds', 'minute', 'polarize_pct_1', 'polarize_pct_6', 'imbalance_buy_sell_flag']\n",
    "        select_columns = list(set(numerical) - set(unselect_columns))\n",
    "        \n",
    "        X_tr_continuous = scaler.fit_transform(X_train[numerical], select_columns, unselect_columns)\n",
    "        X_tr_categorical = X_train[categorical].values\n",
    "\n",
    "        X_ts_continuous = scaler.transform(X_test[numerical])\n",
    "        X_ts_categorical = X_test[categorical].values\n",
    "\n",
    "        print(\"X_train_numerical shape:\",X_tr_continuous.shape)\n",
    "        print(\"X_train_categorical shape:\",X_tr_categorical.shape)\n",
    "        print(\"Y_train shape:\",y_train.shape)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"X_test_numerical shape:\",X_ts_continuous.shape)\n",
    "        print(\"X_test_categorical shape:\",X_ts_categorical.shape)\n",
    "        print(\"Y_test shape:\",y_test.shape)\n",
    "        \n",
    "        # create model\n",
    "        model = create_mlp(len(numerical), num_categorical, embedding_dims, 1, hidden_units, dropout_rates, learning_rate)\n",
    "\n",
    "        print(f\"\\nFitting Model - Holdout\")\n",
    "        model.fit((X_tr_continuous, X_tr_categorical[:, 0:1], X_tr_categorical[:, 1:2], \n",
    "                   X_tr_categorical[:, 2:3]), y_train,\n",
    "                   epochs=epochs, batch_size=batch_size, \n",
    "                   validation_data=((X_ts_continuous, X_ts_categorical[:, 0:1], X_ts_categorical[:, 1:2],\n",
    "                                     X_ts_categorical[:, 2:3]), y_test),\n",
    "                   callbacks=model_checkpoint)\n",
    "    \n",
    "        model.save_weights(ckp_path) # SAVE MODEL WEIGHTS\n",
    "    \n",
    "        pred = model.predict((X_ts_continuous, X_ts_categorical[:, 0:1], X_ts_categorical[:, 1:2],\n",
    "                              X_ts_categorical[:, 2:3]),\n",
    "                              batch_size=batch_size)\n",
    "    \n",
    "        print(\"Train NN Score:\", mean_absolute_error(y_test, pred))\n",
    "\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    rubbish = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdfaf3",
   "metadata": {
    "papermill": {
     "duration": 0.014762,
     "end_time": "2023-11-09T03:16:27.326796",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.312034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### upload kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12eb14",
   "metadata": {},
   "source": [
    "#### dataset init\n",
    "! /home/username/.local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "#### dataset create \n",
    "! /home/username/.local/bin/kaggle datasets create -p {config['model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48089501",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.933464Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02825,
     "end_time": "2023-11-09T03:16:27.370494",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.342244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ./models/20231208_20:39:19/dataset-metadata.json\n",
      "Starting upload for file my_nn_model_15epochV6_0.h5\n",
      "100%|█████████████████████████████████████████| 258k/258k [00:02<00:00, 122kB/s]\n",
      "Upload successful: my_nn_model_15epochV6_0.h5 (258KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/jhk3211/model-nn-version-yongmin-6\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e79e9c71",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936242Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    # \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5670234f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936428Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T08:51:32.716773Z",
     "iopub.status.busy": "2023-12-03T08:51:32.716377Z",
     "iopub.status.idle": "2023-12-03T08:51:32.732190Z",
     "shell.execute_reply": "2023-12-03T08:51:32.731255Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 53.62895,
     "end_time": "2023-11-09T03:17:21.014741",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.385791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config[\"infer_mode\"]:\n",
    "    import optiver2023\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    \n",
    "#     env = optiver2023.make_env()\n",
    "#     iter_test = env.iter_test()\n",
    "\n",
    "    y_min, y_max = -64, 64\n",
    "    qps = []\n",
    "    counter = 0\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # set scaler and features\n",
    "    scaler = QuantileTransformer()\n",
    "\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "    \n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', 'feature_version_yongmin_2',\n",
    "                                                             'feature_version_alvin_1',\n",
    "                                                             'feature_market_cap', 'feature_sector', 'feature_industry'],\n",
    "                                       dependencies=dependencies, infer=True)\n",
    "    feature_engineer.generate_global_features(df)\n",
    "\n",
    "    df = feature_engineer.transform()\n",
    "\n",
    "    df = df.drop(['time_id', 'row_id', 'target'], axis = 1)\n",
    "    \n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "    \n",
    "    # cat - num\n",
    "    categorical = [\"stock_id\"]\n",
    "    numerical = list(set(df.columns) - set(categorical) - set(['target', 'date_id_copy']))\n",
    "    num_categorical = [len(df[col].unique()) for col in categorical]\n",
    "\n",
    "    unselect_columns = ['dow', 'seconds', 'minute', 'polarize_pct_1', 'polarize_pct_6', 'imbalance_buy_sell_flag']\n",
    "    select_columns = list(set(numerical) - set(unselect_columns))\n",
    "    \n",
    "    # build scaler pipeline\n",
    "    scaler.fit(df[select_columns])\n",
    "    \n",
    "    batch_size = 200\n",
    "    hidden_units = [1024, 256, 128, 64, 32] # 5 layer\n",
    "    dropout_rates = [0, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "    learning_rate = 1e-3\n",
    "    embedding_dims = [50, 50, 50, 50]\n",
    "    \n",
    "    # Load Model\n",
    "    final_model = create_mlp(len(numerical), num_categorical, embedding_dims, 1, hidden_units, dropout_rates, learning_rate)\n",
    "    model_path = \"/kaggle/input/model-nn-version-yongmin-6/my_nn_model_15epochV6_0.h5\"\n",
    "    final_model.load_weights(model_path)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        \n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "    \n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(\n",
    "                                  by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        \n",
    "        # feature engineering\n",
    "        feature_engineer = FeatureEngineer(cache, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', 'feature_version_yongmin_2',\n",
    "                                                                    'feature_version_alvin_1', 'feature_market_cap', 'feature_sector', 'feature_industry'],\n",
    "                                           dependencies=dependencies, infer=True)\n",
    "        \n",
    "        cache_df = feature_engineer.transform()\n",
    "        \n",
    "        cache_df = (cache_df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "        \n",
    "        feat = cache_df[-len(test):]\n",
    "        \n",
    "        if 'currently_scored' in feat.columns:\n",
    "            feat = feat.drop(['currently_scored'], axis=1)\n",
    "        \n",
    "        X_num = feat[numerical]\n",
    "        X_num[select_columns] = scaler.transform(X_num[select_columns])\n",
    "        X_cat = feat[categorical].values\n",
    "        \n",
    "        print(\"X_train_numerical shape:\",X_num.shape)\n",
    "        print(\"X_train_categorical shape:\",X_cat.shape)\n",
    "        \n",
    "        # feat = generate_all_features(cache)[-len(test):]\n",
    "        test_predss = np.zeros(feat.shape[0])\n",
    "        \n",
    "        # prediction\n",
    "        inference_prediction = final_model.predict((X_num, X_cat[:,0:1], X_cat[:,1:2], X_cat[:,2:3], X_cat[:,3:4])).ravel()\n",
    "            \n",
    "        test_predss = zero_sum(inference_prediction, test['bid_size'] + test['ask_size'])\n",
    "        \n",
    "        clipped_predictions = np.clip(test_predss, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        \n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        \n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1c5ef6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936534Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-03T08:51:32.737683Z",
     "iopub.status.busy": "2023-12-03T08:51:32.737031Z",
     "iopub.status.idle": "2023-12-03T08:51:32.747022Z",
     "shell.execute_reply": "2023-12-03T08:51:32.746159Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.042958,
     "end_time": "2023-11-09T03:17:21.150593",
     "exception": false,
     "start_time": "2023-11-09T03:17:21.107635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single 1fold final / fianl no\n",
    "# single 1fold final / fianl\n",
    "# single 5fold final / fianl no\n",
    "# single 5fold final / fianl\n",
    "# stacking 1fold final / fianl no\n",
    "# stacking 1fold final / fianl\n",
    "# stacking 5fold final / fianl no\n",
    "# stacking 5fold final / fianl"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4065803,
     "sourceId": 7062189,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yongmin-venv",
   "language": "python",
   "name": "yongmin-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 99.440385,
   "end_time": "2023-11-09T03:17:22.555761",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-09T03:15:43.115376",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
