{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027df110",
   "metadata": {
    "papermill": {
     "duration": 0.01308,
     "end_time": "2023-12-10T13:10:54.017412",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.004332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70380bb4",
   "metadata": {
    "papermill": {
     "duration": 0.01314,
     "end_time": "2023-12-10T13:10:54.043920",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.030780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441c51ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276368Z",
     "start_time": "2023-11-18T04:44:53.963992Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:10:54.073915Z",
     "iopub.status.busy": "2023-12-10T13:10:54.073443Z",
     "iopub.status.idle": "2023-12-10T13:10:54.078477Z",
     "shell.execute_reply": "2023-12-10T13:10:54.077497Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023116,
     "end_time": "2023-12-10T13:10:54.080506",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.057390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732ddaf",
   "metadata": {
    "papermill": {
     "duration": 0.013084,
     "end_time": "2023-12-10T13:10:54.107102",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.094018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 0.2. kaggle api 설치법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead097c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276461Z",
     "start_time": "2023-11-18T04:44:53.998348Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:10:54.136024Z",
     "iopub.status.busy": "2023-12-10T13:10:54.135013Z",
     "iopub.status.idle": "2023-12-10T13:10:54.139109Z",
     "shell.execute_reply": "2023-12-10T13:10:54.138263Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020664,
     "end_time": "2023-12-10T13:10:54.141086",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.120422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e29b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276579Z",
     "start_time": "2023-11-18T04:44:54.001642Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:10:54.173066Z",
     "iopub.status.busy": "2023-12-10T13:10:54.172074Z",
     "iopub.status.idle": "2023-12-10T13:10:54.177043Z",
     "shell.execute_reply": "2023-12-10T13:10:54.176166Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024583,
     "end_time": "2023-12-10T13:10:54.179081",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.154498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7e7f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276667Z",
     "start_time": "2023-11-18T04:44:54.001730Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:10:54.207980Z",
     "iopub.status.busy": "2023-12-10T13:10:54.207573Z",
     "iopub.status.idle": "2023-12-10T13:10:54.212004Z",
     "shell.execute_reply": "2023-12-10T13:10:54.211088Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021436,
     "end_time": "2023-12-10T13:10:54.213949",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.192513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mv ~/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858e09e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276757Z",
     "start_time": "2023-11-18T04:44:54.001870Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:10:54.243054Z",
     "iopub.status.busy": "2023-12-10T13:10:54.242641Z",
     "iopub.status.idle": "2023-12-10T13:10:54.246760Z",
     "shell.execute_reply": "2023-12-10T13:10:54.245774Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021309,
     "end_time": "2023-12-10T13:10:54.249049",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.227740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84b3d5",
   "metadata": {
    "papermill": {
     "duration": 0.013193,
     "end_time": "2023-12-10T13:10:54.276233",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.263040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. config 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8379b6",
   "metadata": {
    "papermill": {
     "duration": 0.013149,
     "end_time": "2023-12-10T13:10:54.365471",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.352322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1. init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d86a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276841Z",
     "start_time": "2023-11-18T04:44:54.045651Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026629,
     "end_time": "2023-12-10T13:10:54.405402",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.378773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference, both\n",
    "KAGGLE_DATASET_NAME = \"model-lgbm-version-yongmin-9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a5aa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.081563Z",
     "start_time": "2023-11-18T04:44:54.045860Z"
    },
    "papermill": {
     "duration": 6.004886,
     "end_time": "2023-12-10T13:11:00.423645",
     "exception": false,
     "start_time": "2023-12-10T13:10:54.418759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import functools\n",
    "import time\n",
    "from numba import njit, prange\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fc62d",
   "metadata": {
    "papermill": {
     "duration": 0.01809,
     "end_time": "2023-12-10T13:11:00.461486",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.443396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.2. train / inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4642164f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.084772Z",
     "start_time": "2023-11-18T04:44:55.081326Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02558,
     "end_time": "2023-12-10T13:11:00.502495",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.476915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.3.2', '2.0.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__, xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eef4d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.084927Z",
     "start_time": "2023-11-18T04:44:55.081831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02187,
     "end_time": "2023-12-10T13:11:00.538152",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.516282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPS = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07653917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.174297Z",
     "start_time": "2023-11-18T04:44:55.082342Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026129,
     "end_time": "2023-12-10T13:11:00.578395",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.552266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2207a4",
   "metadata": {
    "papermill": {
     "duration": 0.013424,
     "end_time": "2023-12-10T13:11:00.605785",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.592361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.3. model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2196d107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.178131Z",
     "start_time": "2023-11-18T04:44:55.160976Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025195,
     "end_time": "2023-12-10T13:11:00.644756",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.619561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"final_mode\": False,  # True : using final model, False : not using final model\n",
    "    \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "    'target': 'target',\n",
    "\n",
    "    'split_method': 'rolling',  # time_series, rolling, blocking, holdout\n",
    "    'n_splits': 3,  # number of splits\n",
    "    'correct': True,  # correct boundary\n",
    "    'gap': 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "\n",
    "    'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "    'train_test_ratio': 0.9,  # train, test ratio\n",
    "\n",
    "    'optuna_random_state': 42,\n",
    "}\n",
    "\n",
    "config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "    \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "    \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc11fc",
   "metadata": {
    "papermill": {
     "duration": 0.016732,
     "end_time": "2023-12-10T13:11:00.675378",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.658646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.4. model heyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0cafcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.178347Z",
     "start_time": "2023-11-18T04:44:55.161340Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026599,
     "end_time": "2023-12-10T13:11:00.719578",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.692979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    \"lgb\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": {\n",
    "                \"objective\": \"mae\",\n",
    "                \"n_estimators\": 6800,\n",
    "                \"num_leaves\": 512,\n",
    "                \"subsample\": 0.34,\n",
    "                \"colsample_bytree\": 0.3,\n",
    "                \"learning_rate\": 0.01,\n",
    "                'max_depth': 15,\n",
    "                \"n_jobs\": 4,\n",
    "                \"device\": \"gpu\",\n",
    "                \"verbosity\": -1,\n",
    "                \"importance_type\": \"gain\",\n",
    "                \"reg_alpha\": 3.23,\n",
    "                \"reg_lambda\": 0.015\n",
    "            }\n",
    "    },\n",
    "\n",
    "    \"xgb\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": {\n",
    "            'booster': 'dart',\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"n_estimators\": 6800,\n",
    "            \"max_depth\": 14,\n",
    "            \"eta\": 0.0073356282482453065,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.30000000000000004,\n",
    "            \"colsample_bylevel\": 0.9,\n",
    "            \"min_child_weight\": 0.4824060812428942,\n",
    "            \"reg_lambda\": 182.50819193990537,\n",
    "            \"reg_alpha\": 0.03171419713574529,\n",
    "            \"gamma\": 0.9162634503670075,\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"n_jobs\": 4,\n",
    "            \"verbosity\": 0,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7364c133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.582Z",
     "start_time": "2023-11-18T04:44:55.161493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026699,
     "end_time": "2023-12-10T13:11:00.760475",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.733776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      " 99%|███████████████████████████████████████▋| 199M/201M [00:08<00:00, 25.2MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:08<00:00, 23.8MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3192910",
   "metadata": {
    "papermill": {
     "duration": 0.014766,
     "end_time": "2023-12-10T13:11:00.793301",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.778535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Global Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e46540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.598960Z",
     "start_time": "2023-11-18T04:45:03.585612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027257,
     "end_time": "2023-12-10T13:11:00.834510",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.807253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed56138f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.642481Z",
     "start_time": "2023-11-18T04:45:03.589182Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.107212,
     "end_time": "2023-12-10T13:11:00.955485",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.848273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecca0206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644285Z",
     "start_time": "2023-11-18T04:45:03.620103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024851,
     "end_time": "2023-12-10T13:11:00.994134",
     "exception": false,
     "start_time": "2023-12-10T13:11:00.969283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7626da08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644943Z",
     "start_time": "2023-11-18T04:45:03.620276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022214,
     "end_time": "2023-12-10T13:11:01.031073",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.008859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6a8d7",
   "metadata": {
    "papermill": {
     "duration": 0.01687,
     "end_time": "2023-12-10T13:11:01.061931",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.045061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 각 클래스의 method는 각자 필요에 따라 추가 해서 사용하면 됩니다. 이때 class의 주석에 method를 추가하고, method의 주석에는 method의 역할을 간단하게 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c546ccf",
   "metadata": {
    "papermill": {
     "duration": 0.014646,
     "end_time": "2023-12-10T13:11:01.093481",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.078835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Pre Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cf5d4",
   "metadata": {
    "papermill": {
     "duration": 0.016762,
     "end_time": "2023-12-10T13:11:01.124483",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.107721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea8b56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.863607Z",
     "start_time": "2023-11-18T04:45:03.632767Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02662,
     "end_time": "2023-12-10T13:11:01.165204",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.138584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    데이터 전처리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        전처리할 데이터\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    handle_missing_data()\n",
    "        결측치 처리\n",
    "    handle_outliers()\n",
    "        이상치 처리\n",
    "    normalize()\n",
    "        정규화\n",
    "    custom_preprocessing()\n",
    "        사용자 정의 전처리\n",
    "    transform()\n",
    "        전처리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, infer=False):\n",
    "        self.data = data  # reduce_mem_usage(data) # reduce_mem_usage 정밀도 훼손함 \n",
    "        self.infer = infer\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def handle_missing_data(self):\n",
    "        # 결측치 처리 코드\n",
    "        self.data = self.data.dropna(subset=[\"target\"]) if self.infer == False else self.data\n",
    "        self.data = self.data.reset_index(drop=True) if self.infer == False else self.data\n",
    "        # self.data.reset_index(drop=True, inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def handle_outliers(self):\n",
    "        # 이상치 처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def normalize(self):\n",
    "        # 정규화 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def custom_preprocessing(self):\n",
    "        # 사용자 정의 전처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self):\n",
    "        # 전처리 수행 코드 (위의 메소드 활용 가능)\n",
    "        self.handle_missing_data()\n",
    "        # self.handle_outliers()\n",
    "        # self.normalize()\n",
    "        # self.custom_preprocessing()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b6897",
   "metadata": {
    "papermill": {
     "duration": 0.013598,
     "end_time": "2023-12-10T13:11:01.193675",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.180077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851d911f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.863864Z",
     "start_time": "2023-11-18T04:45:03.863395Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023165,
     "end_time": "2023-12-10T13:11:01.231333",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.208168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26bd8bbd",
   "metadata": {
    "papermill": {
     "duration": 0.028294,
     "end_time": "2023-12-10T13:11:01.310993",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.282699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91bd7105",
   "metadata": {
    "papermill": {
     "duration": 49.686141,
     "end_time": "2023-12-10T13:11:51.011218",
     "exception": false,
     "start_time": "2023-12-10T13:11:01.325077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 3131/3131 [00:05<00:00, 534.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "all_stock_data = {}\n",
    "\n",
    "for s in tqdm(glob.glob(\"./data/alpha/*.csv\") if MODE == \"train\" else glob.glob(\n",
    "        \"/kaggle/input/nasdaq-stocks-historical-data/alpha/*.csv\"), desc=\"Processing files\"):\n",
    "    stock_df = pd.read_csv(s, dtype={\"ticker\": str})\n",
    "    stock_df.query(\"Date >= '2021-08-05' and Date <= '2023-07-06'\", inplace=True)\n",
    "    if len(stock_df) > 180:\n",
    "        all_stock_data[s[13:-15]] = (stock_df, len(stock_df))\n",
    "\n",
    "reversed_stock_list = [\n",
    "        'MNST', 'WING', 'AXON', 'HON', 'MAR', 'OKTA', 'POOL', 'LRCX', 'YOTA', 'PFG',\n",
    "        'NDAQ', 'COIN', 'AMGN', 'TER', 'ADBE', 'ABNB', 'ZBRA', 'KLAC', 'ZI', 'ALNY',\n",
    "        'ULTA', 'SSNC', 'ON', 'SWKS', 'AKAM', 'ASML', 'PPBI', 'QRVO', 'FANG', 'ORLY',\n",
    "        'LNT', 'AGRX', 'NTAP', 'CROX', 'REGN', 'ROST', 'DLTR', 'ADP', 'EMCG', 'CTAS',\n",
    "        'CZR', 'NVDA', 'SAIA', 'JKHY', 'FOSLL', 'MSFT', 'TECH', 'TXRH', 'WDAY', 'FITB',\n",
    "        'MTCH', 'ROKU', 'CINF', 'EBAY', 'SNPS', 'FAST', 'ETSY', 'IDXX', 'INTU', 'ZG',\n",
    "        'CRWD', 'LYFT', 'RGEN', 'LKQ', 'MKTX', 'EXC', 'LBRDK', 'MRNA', 'PAYX', 'SOFI',\n",
    "        'BYND', 'EQIX', 'ADI', 'GEN', 'ALGN', 'CDNS', 'HAS', 'VRTX', 'HOOD', 'WBD',\n",
    "        'TXG', 'SGEN', 'OPEN', 'INTC', 'GOOG', 'CAR', 'UPST', 'LSCC', 'NFLX', 'ENTG',\n",
    "        'FFIV', 'DOCU', 'MSTR', 'ZION', 'PCTY', 'AMD', 'MRVL', 'NBIX', 'JBLU', 'PARA',\n",
    "        'MQ', 'FCNCA', 'TEAM', 'ZS', 'WBA', 'MDLZ', 'TRMB', 'PODD', 'SEDG', 'CSX',\n",
    "        'TMUS', 'SPWR', 'AAPL', 'LULU', 'LPLA', 'ILMN', 'CDW', 'GDS', 'MELI', 'MASI',\n",
    "        'FOXA', 'KDP', 'AAL', 'GILD', 'ASO', 'UTHR', 'MU', 'MDB', 'WDC', 'CFLT',\n",
    "        'SBUX', 'INCY', 'TSCO', 'ISRG', 'VTRS', 'DKNG', 'LITE', 'TTWO', 'SMCI', 'EXPE',\n",
    "        'VRTS', 'AMAT', 'AVGO', 'TLRY', 'PCAR', 'CG', 'MIDD', 'APA', 'LNT', 'VRSK',\n",
    "        'PANW', 'CSCO', 'SBAC', 'HTZ', 'DBX', 'CHKEW', 'LCID', 'ADSK', 'APLS', 'STLD',\n",
    "        'PEP', 'PTON', 'ENPH', 'COST', 'CPRT', 'HST', 'KHC', 'CHRW', 'AMZN', 'ANSS',\n",
    "        'HOLX', 'TROW', 'APP', 'FIVE', 'AFRM', 'GOOGL', 'FTNT', 'SWAV', 'ZM', 'META',\n",
    "        'GH', 'JBHT', 'UAL', 'MCHP', 'DDOG', 'ODFL', 'CTSH', 'EA', 'RUN', 'CSGP',\n",
    "        'DXCM', 'TSLA', 'PTC', 'PYPL', 'PENN', 'XEL', 'XRAY', 'SPLK', 'CMCSA', 'BKR'\n",
    "]\n",
    "\n",
    "stock_list_df = pd.read_csv('./data/nasdaq-screener/nasdaq_screener_1701158836955.csv') if MODE == \"train\" else pd.read_csv(\n",
    "    '/kaggle/input/nasdaq-screener/nasdaq_screener_1701158836955.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66eb1f1d",
   "metadata": {
    "papermill": {
     "duration": 0.055938,
     "end_time": "2023-12-10T13:11:51.110754",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.054816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_stock_info(df, data, column_name):  # column_name = \"Market Cap\", \"Sector\", \"Industry\"\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    if column_name != \"Market Cap\":\n",
    "        stock_list_df[column_name] = le.fit_transform(stock_list_df[column_name])\n",
    "\n",
    "    df[f'{column_name}'] = -1\n",
    "\n",
    "    for idx, ticker in enumerate(reversed_stock_list):\n",
    "        stock_id_indices = data[data['stock_id'] == idx].index\n",
    "        if ticker in stock_list_df[\"Symbol\"].values:\n",
    "            value = stock_list_df[stock_list_df[\"Symbol\"] == ticker][column_name].iloc[0]\n",
    "            df.loc[stock_id_indices, f'{column_name}'] = value\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2dc808",
   "metadata": {
    "papermill": {
     "duration": 0.058888,
     "end_time": "2023-12-10T13:11:51.214482",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.155594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "_weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa8e114d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.937718Z",
     "start_time": "2023-11-18T04:45:03.863730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.095718,
     "end_time": "2023-12-10T13:11:51.353411",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.257693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_0(self, *args, version_name=\"feature_version_yongmin_0\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df['dow'] = self.data[\"date_id\"] % 5\n",
    "        df['seconds'] = self.data['seconds_in_bucket'] % 60\n",
    "        df['minute'] = self.data['seconds_in_bucket'] // 60\n",
    "        df['time_to_market_close'] = 540 - self.data['seconds_in_bucket']\n",
    "    \n",
    "        self.data[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "    \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_1(self, *args, version_name=\"feature_version_yongmin_1\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        self.data[\"stock_weights\"] = self.data[\"stock_id\"].map(_weights)\n",
    "        self.data[\"weighted_wap\"] = self.data[\"stock_weights\"] * self.data[\"wap\"]\n",
    "        \n",
    "        self.data['cum_wap'] = self.data.groupby(['stock_id'])['wap'].cumprod()\n",
    "        self.data['cum_weighted_wap'] = self.data.groupby(['stock_id'])['weighted_wap'].cumprod()\n",
    "    \n",
    "        for i in [1, 3, 6, 12]:\n",
    "            df[f\"cum_wap_ma_{i}\"] = self.data.groupby(['stock_id'])['cum_wap'].rolling(i).mean().values\n",
    "            df[f'wap_momentum_{i}'] = self.data.groupby('stock_id')['weighted_wap'].pct_change(periods=i)\n",
    "\n",
    "        self.data[\"imbalance_momentum\"] = self.data.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / self.data['matched_size']\n",
    "        self.data[\"imbalance_momentum_r\"] = self.data.groupby(['stock_id'])['matched_size'].diff(periods=1) / self.data['imbalance_size']\n",
    "        self.data[\"anb_imb_momentum\"] =self.data.groupby(['stock_id'])['ask_size'].diff(periods=1) / self.data['bid_size']\n",
    "        self.data[\"anb_imb_momentum_r\"] =self.data.groupby(['stock_id'])['bid_size'].diff(periods=1) / self.data['ask_size']\n",
    "        self.data[\"imb_momentum_spread\"] = self.data[\"imbalance_momentum\"] - self.data[\"anb_imb_momentum\"]\n",
    "        self.data[\"price_spread\"] = self.data[\"ask_price\"] - self.data[\"bid_price\"]\n",
    "        \n",
    "        self.data[\"mid_price\"] = self.data.eval(\"(ask_price + bid_price) / 2\")\n",
    "        self.data[\"liquidity_imbalance\"] = self.data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n",
    "        self.data[\"matched_imbalance\"] = self.data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n",
    "        self.data[\"size_imbalance\"] = self.data.eval(f\"bid_size / ask_size+{EPS}\")\n",
    "        self.data[\"size_gini\"] = self.data.eval(f\"ask_size / (bid_size+ask_size+{EPS})\")\n",
    "        self.data[\"size_gini_2\"] = self.data.eval(f\"matched_size / (matched_size+imbalance_size+{EPS})\")\n",
    "\n",
    "        \n",
    "        df[\"size_spread_intensity\"] = self.data.groupby(['stock_id'])['imb_momentum_spread'].diff()\n",
    "        df[\"spread_intensity\"] = self.data.groupby(['stock_id'])['price_spread'].diff()\n",
    "        df['price_pressure'] = self.data['imbalance_size'] * (self.data['ask_price'] - self.data['bid_price'])\n",
    "        \n",
    "        self.data['market_urgency_like'] = self.data['imb_momentum_spread'] * self.data['liquidity_imbalance']\n",
    "        self.data['market_urgency'] = self.data['price_spread'] * self.data['liquidity_imbalance']\n",
    "        df['depth_pressure'] = (self.data['ask_size'] - self.data['bid_size']) * (self.data['far_price'] - self.data['near_price'])\n",
    "        \n",
    "        df['spread_depth_ratio'] = (self.data['ask_price'] - self.data['bid_price']) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        self.data['mid_price_movement'] = self.data['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "        \n",
    "        df['micro_price'] = ((self.data['bid_price'] * self.data['ask_size']) + (self.data['ask_price'] * self.data['bid_size'])) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        df['relative_spread'] = (self.data['ask_price'] - self.data['bid_price']) / self.data['wap']\n",
    "\n",
    "\n",
    "        df['mid_price*volume'] = self.data['mid_price_movement'] * self.data['volume']\n",
    "        df['harmonic_imbalance'] = 2 / ((1 / self.data[\"bid_size\"] ) + (1 / self.data[\"ask_size\"]))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_2(self, *args, version_name=\"feature_version_yongmin_2\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "        \n",
    "        for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "            self.data[f\"all_prices_{func}\"] = self.data[prices].agg(func, axis=1)\n",
    "            self.data[f\"all_sizes_{func}\"] = self.data[sizes].agg(func, axis=1)\n",
    "\n",
    "        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "            for window in [1,3,6,9,12]:\n",
    "                self.data[f\"{col}_shift_{window}\"] = self.data.groupby('stock_id')[col].shift(window)\n",
    "                self.data[f\"{col}_ret_{window}\"] = self.data.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "        # Calculate diff features for specific columns\n",
    "        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "            for window in [1,3,6,9,12]:\n",
    "                self.data[f\"{col}_diff_{window}\"] = self.data.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "        for window in [1,3,6,9,12]:\n",
    "            self.data[f'price_change_diff_{window}'] = self.data[f'bid_price_diff_{window}'] - self.data[f'ask_price_diff_{window}']\n",
    "            self.data[f'size_change_diff_{window}'] = self.data[f'bid_size_diff_{window}'] - self.data[f'ask_size_diff_{window}']\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_3(self, *args, version_name=\"feature_version_yongmin_3\"):\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        for i in ['wap', 'ask_size', 'bid_size', 'weighted_wap', 'price_spread']:\n",
    "            _stock_wise_std = self.data.groupby(['date_id'])[i].std()\n",
    "            _stock_wise_pct_change = self.data.groupby(['date_id'])[i].pct_change()\n",
    "            \n",
    "            df[f'stock_wise_{i}_std'] = self.data['date_id'].map(_stock_wise_std)\n",
    "            df[f'stock_wise_{i}_pct'] = self.data['date_id'].map(_stock_wise_pct_change)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_4(self, *args, version_name=\"feature_version_yongmin_4\"):\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        bid_price_norm, ask_price_norm = (self.data[\"bid_price\"] / self.data[\"bid_price\"].max()), (self.data[\"ask_price\"] / self.data[\"ask_price\"].max())\n",
    "        df['ask_bid_norm'] = self.data[\"ask_size\"] * bid_price_norm\n",
    "        df['bid_ask_norm'] = self.data[\"bid_size\"] * ask_price_norm\n",
    "    \n",
    "        df['cross_spread'] = df['bid_ask_norm'] - df['ask_bid_norm']\n",
    "        df['cross_imbalance'] = df.eval(\"(bid_ask_norm-ask_bid_norm)/(bid_ask_norm+ask_bid_norm)\")\n",
    "        df['cross_imbalance_ratio'] = df['bid_ask_norm'] / df['ask_bid_norm']\n",
    "        df['cross_urgency'] = df['cross_spread'] * df['cross_imbalance']\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_1(self, *args, version_name=\"feature_version_alvin_1\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = self.data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n",
    "\n",
    "        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "            triplet_feature = calculate_triplet_imbalance_numba(c, self.data)\n",
    "            df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_market_cap(self, *args, version_name=\"feature_market_cap\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Market Cap\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_sector(self, *args, version_name=\"feature_sector\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Sector\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_industry(self, *args, version_name=\"feature_industry\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Industry\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        final_data = concat_df\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775c5b0",
   "metadata": {
    "papermill": {
     "duration": 0.043932,
     "end_time": "2023-12-10T13:11:51.440552",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.396620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c7c749c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.939923Z",
     "start_time": "2023-11-18T04:45:03.937632Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.272371,
     "end_time": "2023-12-10T13:11:51.756704",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.484333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \"\"\"\n",
    "    데이터 분리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        데이터 분리 방식\n",
    "    n_splits : int\n",
    "        데이터 분리 개수\n",
    "    correct : bool\n",
    "        데이터 분리 시 boundary를 맞출지 여부\n",
    "    initial_fold_size_ratio : float\n",
    "        초기 fold size 비율\n",
    "    train_test_ratio : float\n",
    "        train, test 비율\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    split()\n",
    "        데이터 분리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n",
    "                 overlap=True, train_start=0,\n",
    "                 train_end=390, valid_start=391, valid_end=480):\n",
    "        self.method = method\n",
    "        self.n_splits = n_splits\n",
    "        self.correct = correct\n",
    "        self.initial_fold_size_ratio = initial_fold_size_ratio\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "\n",
    "        self.gap = gap\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # only for holdout method\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.valid_start = valid_start\n",
    "        self.valid_end = valid_end\n",
    "\n",
    "        self.target = config[\"target\"]\n",
    "\n",
    "        self.boundaries = []\n",
    "\n",
    "    def split(self, data):\n",
    "        #self.data = reduce_mem_usage(data)\n",
    "        self.data = data\n",
    "        self.all_dates = self.data['date_id_copy'].unique()\n",
    "        if self.method == \"time_series\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Time series split method only works with n_splits > 1\")\n",
    "            return self._time_series_split(data)\n",
    "        elif self.method == \"rolling\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"blocking\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n",
    "            self.initial_fold_size_ratio = 1.0 / self.n_splits\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"holdout\":\n",
    "            if self.n_splits != 1:\n",
    "                raise ValueError(\"Holdout method only works with n_splits=1\")\n",
    "            return self._holdout_split(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "    def _correct_boundary(self, data, idx, direction=\"forward\"):\n",
    "        # Correct the boundary based on date_id_copy\n",
    "        original_idx = idx\n",
    "        if idx == 0 or idx == len(data) - 1:\n",
    "            return idx\n",
    "        if direction == \"forward\":\n",
    "            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx += 1\n",
    "        elif direction == \"backward\":\n",
    "            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx -= 1\n",
    "            idx += 1  # adjust to include the boundary\n",
    "        return idx\n",
    "\n",
    "    def _time_series_split(self, data):\n",
    "        n = len(data)\n",
    "        initial_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n",
    "        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n",
    "            train_size = fold_size - initial_test_size\n",
    "\n",
    "            if self.correct:\n",
    "                train_size = self._correct_boundary(data, train_size, \"forward\")\n",
    "                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n",
    "            else:\n",
    "                end_of_test = train_size + initial_test_size\n",
    "\n",
    "            train_slice = data.iloc[:train_size]\n",
    "            test_slice = data.iloc[train_size:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "\n",
    "    def _rolling_split(self, data):\n",
    "        n = len(data)\n",
    "        total_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n",
    "        gap_size = int(total_fold_size * self.gap)\n",
    "        train_size = total_fold_size - test_size\n",
    "        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n",
    "\n",
    "        end_of_test = n - 1\n",
    "        start_of_test = end_of_test - test_size\n",
    "        end_of_train = start_of_test - gap_size\n",
    "        start_of_train = end_of_train - train_size\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            if self.correct:\n",
    "                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n",
    "                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n",
    "                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n",
    "                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n",
    "\n",
    "            train_slice = data[start_of_train:end_of_train]\n",
    "            test_slice = data[start_of_test:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[0],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "            start_of_train = max(start_of_train - rolling_increment, 0)\n",
    "            end_of_train -= rolling_increment\n",
    "            start_of_test -= rolling_increment\n",
    "            end_of_test -= rolling_increment\n",
    "\n",
    "    def _holdout_split(self, data):\n",
    "        # train_start ~ train_end : 학습 데이터 기간\n",
    "        # valid_start ~ valid_end : 검증 데이터 기간\n",
    "        # 학습 및 검증 데이터 분리\n",
    "        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n",
    "        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n",
    "\n",
    "        train_slice = data[train_mask]\n",
    "        valid_slice = data[valid_mask]\n",
    "\n",
    "        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_train = train_slice[self.target]\n",
    "        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_valid = valid_slice[self.target]\n",
    "\n",
    "        self.boundaries.append((\n",
    "            train_slice['date_id_copy'].iloc[0],\n",
    "            train_slice['date_id_copy'].iloc[-1],\n",
    "            valid_slice['date_id_copy'].iloc[0],\n",
    "            valid_slice['date_id_copy'].iloc[-1]\n",
    "        ))\n",
    "        yield X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    def visualize_splits(self):\n",
    "        print(\"Visualizing Train/Test Split Boundaries\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n",
    "            train_width = train_end - train_start + 1\n",
    "            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n",
    "            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n",
    "                     color='black', fontsize=8)\n",
    "\n",
    "            test_width = test_end - test_start + 1\n",
    "            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n",
    "            if test_width > 0:\n",
    "                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n",
    "                         color='black', fontsize=8)\n",
    "\n",
    "        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n",
    "        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n",
    "        plt.xlabel(\"date_id_copy\")\n",
    "        plt.title(\"Train/Test Split Boundaries\")\n",
    "        plt.grid(axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090d22b",
   "metadata": {
    "papermill": {
     "duration": 0.042549,
     "end_time": "2023-12-10T13:11:51.843965",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.801416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b00b2564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.941854Z",
     "start_time": "2023-11-18T04:45:03.938558Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.059221,
     "end_time": "2023-12-10T13:11:51.946865",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.887644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=5000):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        #         weights = [trial.suggest_float(f\"weight{n}\", -2, 3) for n in range(len(y_preds))]\n",
    "        weights = [max(0, trial.suggest_float(f\"weight{n}\", -2, 3)) for n in range(len(y_preds))]\n",
    "        # Calculate the weighted prediction\n",
    "        if sum(weights) == 0:\n",
    "            num_models = len(y_preds)\n",
    "            weights = [1 / num_models] * num_models\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "        auc_score = mean_absolute_error(y_true, weighted_pred)\n",
    "        #         log_loss_score=log_loss(y_true, weighted_pred)\n",
    "        return auc_score  #/log_loss_score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\",\n",
    "                                         direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "\n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2feb2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:04.014235Z",
     "start_time": "2023-11-18T04:45:03.939103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.08014,
     "end_time": "2023-12-10T13:11:52.069318",
     "exception": false,
     "start_time": "2023-12-10T13:11:51.989178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# V2 ModlePipeline\n",
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.model_mode = config[\"model_mode\"]  # \"single\" or \"stacking\"\n",
    "        self.final_mode = config[\"final_mode\"]  # False\n",
    "        self.train_mode = config[\"train_mode\"]  # True\n",
    "        self.mae_mode = config[\"mae_mode\"]  # False\n",
    "        self.model_names = config[\"model_name\"]  # [\"lgb\", \"xgb\", \"pytorch_cnn\"]\n",
    "        self.n_splits = config[\"n_splits\"]  # 5\n",
    "        self.model_dir = config[\"model_dir\"]  # \"./model\"\n",
    "        self.optuna_random_state = config[\"optuna_random_state\"]  # 42\n",
    "\n",
    "        self.best_iterate_ratio = config[\"best_iterate_ratio\"]  # 1.2\n",
    "\n",
    "        self.models = []  # [] fold 에서 모델 저장\n",
    "        self.models_list = []  # [[],] 전체 fold 에서 모델 저장\n",
    "        self.single_final_model = None\n",
    "\n",
    "        self.predictions = []  # [] fold 에서 모델 예측값 저장\n",
    "        self.single_model_mae = []  # [] fold 에서 mae 값 저장\n",
    "\n",
    "        self.optuna_weights = []  # [] fold 에서 optuna weights 저장\n",
    "\n",
    "        self.inference_models = {}\n",
    "        self.inference_prediction = None\n",
    "\n",
    "    def train(self, idx, X_train, y_train, X_valid=None, y_valid=None, use_early_stopping=True, best_iteration=None):\n",
    "        self.models = []\n",
    "        for model_name in self.model_names:\n",
    "            model_cls = models_config[model_name][\"model\"]\n",
    "            params = models_config[model_name][\"params\"]\n",
    "            if best_iteration is not None:\n",
    "                params[\"n_estimators\"] = best_iteration[model_name]\n",
    "            model = model_cls(**params)\n",
    "            print(f\"\\n\\n================== Training {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "            if \"lgb\" in model_name:\n",
    "                fit_params = {\n",
    "                    \"callbacks\": [lgb.callback.log_evaluation(period=100)]\n",
    "                }\n",
    "                if use_early_stopping:\n",
    "                    fit_params[\"callbacks\"].append(lgb.callback.early_stopping(stopping_rounds=100))\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    fit_params[\"eval_set\"] = [(X_valid, y_valid)]\n",
    "\n",
    "                model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "            elif \"xgb\" in model_name:\n",
    "                fit_params = {\n",
    "                    \"eval_set\": [(X_valid, y_valid)],\n",
    "                    \"eval_metric\": \"mae\",\n",
    "                    \"verbose\": 100\n",
    "                }\n",
    "                if use_early_stopping:\n",
    "                    fit_params[\"early_stopping_rounds\"] = 100\n",
    "\n",
    "                model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "            elif model_name == \"pytorch_cnn\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model name\")\n",
    "            print(f\"Successfully trained {model_name} ({idx}/{config['n_splits']})\")\n",
    "            self.models.append(model)\n",
    "            # joblib.dump(model, f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "            # print(f\"Successfully saved model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "        self.models_list.append(self.models)\n",
    "\n",
    "    def predict(self, idx, X_test, infer=False):\n",
    "        self.predictions = []\n",
    "        if infer:\n",
    "            if not self.mae_mode:\n",
    "                for model_name in self.model_names:\n",
    "                    # print(\n",
    "                    #     f\"\\n\\n================== Inference each model {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "                    model = self.inference_models[f\"{self.model_dir}/{idx}_{model_name}.pkl\"] if MODE != \"both\" else \\\n",
    "                        self.models_list[idx][self.model_names.index(model_name)]\n",
    "                    self.predictions.append(model.predict(X_test))\n",
    "                    self.inference_prediction = self.predictions[0]  # for sigle model (non stakcing)\n",
    "                    # print(f\"Successfully inference {model_name} ({idx}/{config['n_splits']})\")\n",
    "            else:\n",
    "                # single model mae\n",
    "                model = self.single_final_model[0]\n",
    "                self.predictions.append(model.predict(X_test))\n",
    "                self.inference_prediction = self.predictions[0]  # for sigle model (mae)\n",
    "\n",
    "        else:\n",
    "            for model_name, model in zip(self.model_names, self.models):\n",
    "                print(\n",
    "                    f\"\\n\\n================== Predict each model {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "                self.predictions.append(model.predict(X_test))\n",
    "                print(f\"Successfully predicted {model_name} ({idx}/{config['n_splits']})\")\n",
    "                if self.mae_mode:\n",
    "                    score = mean_absolute_error(y_test, self.predictions[0])\n",
    "                    print(f\"Score for single model {model_name} ({idx}/{config['n_splits']}): {score}\")\n",
    "                    self.single_model_mae.append(score)\n",
    "\n",
    "    def stacking(self, idx, y_test=None, infer=False):\n",
    "        # stacking 코드\n",
    "        optuna = OptunaWeights(random_state=self.optuna_random_state)\n",
    "        if infer:\n",
    "            self.inference_prediction = None\n",
    "            # print(f\"\\n\\n================== Inference stacking ({idx}/{config['n_splits']})==================\")\n",
    "            optuna.weights = self.optuna_weights[idx]\n",
    "            self.inference_prediction = optuna.predict(self.predictions)\n",
    "            print(f\"Successfully inference stacking ({idx}/{config['n_splits']})\")\n",
    "        else:\n",
    "            print(f\"\\n\\n================== Stacking ({idx}/{config['n_splits']})==================\")\n",
    "            y_test_pred = optuna.fit_predict(y_test.values, self.predictions)\n",
    "            score = mean_absolute_error(y_test, y_test_pred)\n",
    "            print(f\"Score for stacking ({idx}/{config['n_splits']}): {score}\")\n",
    "            self.optuna_weights.append(optuna.weights)\n",
    "            print(f\"Successfully stacking ({idx}/{config['n_splits']})\")\n",
    "\n",
    "    def final_train(self, data):\n",
    "        best_iterations = {name: [] for name in self.model_names}\n",
    "\n",
    "        for n in range(self.n_splits):\n",
    "            for model_name in self.model_names:\n",
    "                model = self.models_list[n][self.model_names.index(model_name)]\n",
    "                if \"lgb\" in model_name:\n",
    "                    best_iterations[model_name].append(model.best_iteration_)\n",
    "                elif \"xgb\" in model_name:\n",
    "                    best_iterations[model_name].append(model.get_booster().best_iteration)\n",
    "\n",
    "        average_best_iterations = {name: int(int(np.mean(iterations)) * self.best_iterate_ratio) for name, iterations in\n",
    "                                   best_iterations.items()}\n",
    "        for model_name, average in average_best_iterations.items():\n",
    "            print(f\"Average best iteration for {model_name.upper()} models: {average}\")\n",
    "\n",
    "        # final model 로 학습할거라 이전 모델 초기화\n",
    "        self.models_list = []\n",
    "        self.models = []\n",
    "        self.predictions = []\n",
    "        if self.model_mode == \"stacking\":\n",
    "            splitter = Splitter(method=\"holdout\", n_splits=1, correct=True)\n",
    "            for idx, (X_final_train, y_final_train, X_final_test, y_final_test) in enumerate(splitter.split(data)):\n",
    "                print(X_final_train.shape, y_final_train.shape, X_final_test.shape, y_final_test.shape)\n",
    "                self.train(idx, X_final_train, y_final_train, X_final_test, y_final_test,\n",
    "                           use_early_stopping=False,\n",
    "                           best_iteration=average_best_iterations)\n",
    "                self.predict(idx, X_final_test)\n",
    "                self.stacking(idx, y_final_test)\n",
    "        else:\n",
    "            X_final_train = data.drop(columns=[config[\"target\"], 'date_id_copy'])\n",
    "            y_final_train = data[config[\"target\"]]\n",
    "            print(X_final_train.shape, y_final_train.shape)\n",
    "            self.train(0, X_final_train, y_final_train,\n",
    "                       use_early_stopping=False,\n",
    "                       best_iteration=average_best_iterations)\n",
    "\n",
    "    def save_models(self):\n",
    "        # 모델 저장\n",
    "        if MODE == \"train\":\n",
    "            for idx in range(config[\"inference_n_splits\"]):\n",
    "                for n_model, model_name in enumerate(self.model_names):\n",
    "                    model = self.models_list[idx][n_model]\n",
    "                    joblib.dump(model, f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "                    print(f\"Successfully saved model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "            if self.mae_mode:\n",
    "                # select best model\n",
    "                max_idx = np.argmin(self.single_model_mae)\n",
    "                self.single_final_model = self.models_list[max_idx]\n",
    "                print(f\"The best model is {self.model_names[0]} ({max_idx}/{config['n_splits']})\")\n",
    "                joblib.dump(self.single_final_model, f\"{self.model_dir}/single_final_model.pkl\")\n",
    "                print(f\"Successfully saved single model mae ({self.model_dir}/single_final_model.pkl)\")\n",
    "\n",
    "    def save_optuna_weights(self):\n",
    "        # optuna weights 저장\n",
    "        if MODE == \"train\":\n",
    "            if self.model_mode == \"stacking\":\n",
    "                joblib.dump(self.optuna_weights, f\"{self.model_dir}/optuna_weights.pkl\")\n",
    "                print(f\"Successfully saved optuna weights ({self.model_dir}/optuna_weights.pkl)\")\n",
    "\n",
    "    def load_models(self):\n",
    "        # 모델 불러오기\n",
    "        if MODE == \"inference\":\n",
    "            for idx in range(config[\"inference_n_splits\"]):\n",
    "                for model_name in self.model_names:\n",
    "                    model = joblib.load(f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "                    print(f\"Successfully loaded model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "                    self.inference_models[f\"{self.model_dir}/{idx}_{model_name}.pkl\"] = model\n",
    "            if self.mae_mode:\n",
    "                self.single_final_model = joblib.load(f\"{self.model_dir}/single_final_model.pkl\")\n",
    "                print(f\"Successfully loaded single model mae ({self.model_dir}/single_final_model.pkl)\")\n",
    "\n",
    "    def load_optuna_weights(self):\n",
    "        # optuna weights 불러오기\n",
    "        if MODE == \"inference\":\n",
    "            if self.model_mode == \"stacking\":\n",
    "                self.optuna_weights = joblib.load(f\"{self.model_dir}/optuna_weights.pkl\")\n",
    "                print(f\"Successfully loaded optuna weights ({self.model_dir}/optuna_weights.pkl)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ceb72",
   "metadata": {
    "papermill": {
     "duration": 0.042126,
     "end_time": "2023-12-10T13:11:52.154039",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.111913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Main\n",
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fd9c4e0-f237-4432-9d63-2a32a54ed1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07233b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:04.014440Z",
     "start_time": "2023-11-18T04:45:03.999301Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.051011,
     "end_time": "2023-12-10T13:11:52.247477",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.196466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 피쳐 엔지니어링 할 함수에 args가 들어간다면 dependencies에 추가\n",
    "dependencies = {\n",
    "    # \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f0079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:58:30.576229Z",
     "start_time": "2023-11-18T04:46:09.604461Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.058511,
     "end_time": "2023-12-10T13:11:52.348920",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.290409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed handle_missing_data, Elapsed time: 0.31 seconds, shape((5237892, 15))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 0.31 seconds, shape((5237892, 15))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.64 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_0, Elapsed time: 0.07 seconds, shape((5237892, 4))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_1, Elapsed time: 8.65 seconds, shape((5237892, 17))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_2, Elapsed time: 18.10 seconds, shape((5237892, 0))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_3, Elapsed time: 1.80 seconds, shape((5237892, 10))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_4, Elapsed time: 0.16 seconds, shape((5237892, 6))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_alvin_1, Elapsed time: 1.32 seconds, shape((5237892, 23))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_market_cap, Elapsed time: 7.32 seconds, shape((5237892, 1))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_industry, Elapsed time: 7.28 seconds, shape((5237892, 1))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 44.71 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 2.78 seconds, shape((5237892, 185))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 56.90 seconds, shape((5237892, 186))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "(3759689, 184) (3759689,) (417999, 184) (417999,)\n",
      "\n",
      "\n",
      "================== Training lgb (0/3)==================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.81594\n",
      "[200]\tvalid_0's l1: 5.79337\n",
      "[300]\tvalid_0's l1: 5.78423\n",
      "[400]\tvalid_0's l1: 5.77744\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = ModelPipeline()\n",
    "\n",
    "if config[\"train_mode\"]:\n",
    "    # 데이터 불러오기\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "\n",
    "    df = df.drop(['row_id', 'time_id'], axis=1)\n",
    "\n",
    "    # 데이터 전처리\n",
    "    data_processor = DataPreprocessor(df)\n",
    "    df = data_processor.transform()\n",
    "\n",
    "    # 사용할 피쳐 엔지니어링 함수 선택\n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', \n",
    "                                                             'feature_version_yongmin_2', 'feature_version_yongmin_3', 'feature_version_yongmin_4',\n",
    "                                                             'feature_version_alvin_1', 'feature_market_cap', 'feature_industry'],\n",
    "                                       \n",
    "                                       dependencies=dependencies)\n",
    "    \n",
    "    feature_engineer.generate_global_features(df)\n",
    "    \n",
    "    df = feature_engineer.transform()  # 맨 처음에는 save=True 돌렸으면, 다음부턴 transform(load=True)로 바꾸면된 \n",
    "    \n",
    "    splitter = Splitter(method=config[\"split_method\"], n_splits=config[\"n_splits\"], correct=config[\"correct\"],\n",
    "                        initial_fold_size_ratio=config[\"initial_fold_size_ratio\"],\n",
    "                        train_test_ratio=config[\"train_test_ratio\"], gap=config[\"gap\"])\n",
    "    \n",
    "    for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(df)):\n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "        if \"date_id_copy\" in X_train.columns:\n",
    "            X_train = X_train.drop(['date_id_copy'], axis=1)\n",
    "            X_test = X_test.drop(['date_id_copy'], axis=1)\n",
    "        \n",
    "        model_pipeline.train(idx, X_train, y_train, X_test, y_test)\n",
    "        model_pipeline.predict(idx, X_test)\n",
    "\n",
    "        if config[\"final_mode\"]:\n",
    "            model_pipeline.final_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f01ef4-b6d9-45d8-87c1-927daf840558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"train_mode\"]:\n",
    "    model_pipeline.save_models()\n",
    "    model_pipeline.save_optuna_weights()\n",
    "    splitter.visualize_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffe0d0",
   "metadata": {
    "papermill": {
     "duration": 0.043596,
     "end_time": "2023-12-10T13:11:52.435110",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.391514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### upload kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba59372",
   "metadata": {
    "papermill": {
     "duration": 0.042535,
     "end_time": "2023-12-10T13:11:52.520773",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.478238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### dataset init\n",
    "! /home/username/.local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "#### dataset create \n",
    "! /home/username/.local/bin/kaggle datasets create -p {config['model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31004db",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.933464Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055128,
     "end_time": "2023-12-10T13:11:52.620291",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.565163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d948bfb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936242Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.05167,
     "end_time": "2023-12-10T13:11:52.714437",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.662767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    # \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e94d7199",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936428Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:11:52.802734Z",
     "iopub.status.busy": "2023-12-10T13:11:52.802298Z",
     "iopub.status.idle": "2023-12-10T13:16:55.590918Z",
     "shell.execute_reply": "2023-12-10T13:16:55.589942Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 302.835848,
     "end_time": "2023-12-10T13:16:55.593575",
     "exception": false,
     "start_time": "2023-12-10T13:11:52.757727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model (/kaggle/input/model-lgbm-version-yongmin-5/0_lgb.pkl)\n",
      "Successfully loaded single model mae (/kaggle/input/model-lgbm-version-yongmin-5/single_final_model.pkl)\n",
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 2.0898277759552\n",
      "20 qps: 1.941500461101532\n",
      "30 qps: 1.8946048021316528\n",
      "40 qps: 1.8768567323684693\n",
      "50 qps: 1.859838604927063\n",
      "60 qps: 1.8536233941713969\n",
      "70 qps: 1.8488979646137782\n",
      "80 qps: 1.8477144569158555\n",
      "90 qps: 1.8426805708143446\n",
      "100 qps: 1.840173692703247\n",
      "110 qps: 1.8381189996545966\n",
      "120 qps: 1.8351737995942434\n",
      "130 qps: 1.8339970368605394\n",
      "140 qps: 1.8303761260850089\n",
      "150 qps: 1.8282303428649902\n",
      "160 qps: 1.8279522195458413\n",
      "The code will take approximately 2.0939 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "if config[\"infer_mode\"]:\n",
    "    import optiver2023\n",
    "\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    y_min, y_max = -64, 64\n",
    "    qps = []\n",
    "    counter = 0\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    model_pipeline.load_models()\n",
    "    model_pipeline.load_optuna_weights()\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        \n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(\n",
    "                by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "            \n",
    "        # preprocessing\n",
    "        data_processor = DataPreprocessor(cache, infer=True)\n",
    "        cache_df = data_processor.transform()\n",
    "        \n",
    "        # feature engineering\n",
    "        feature_engineer = FeatureEngineer(cache_df, infer=True, \n",
    "                                           feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', \n",
    "                                                             'feature_version_yongmin_2', 'feature_version_yongmin_3', 'feature_version_yongmin_4',\n",
    "                                                             'feature_version_alvin_1', 'feature_market_cap', 'feature_sector', 'feature_industry'],\n",
    "                                           dependencies=dependencies)\n",
    "        \n",
    "        cache_df = feature_engineer.transform()\n",
    "        feat = cache_df[-len(test):]\n",
    "        \n",
    "        feat = feat.drop(['row_id', 'currently_scored'], axis=1)\n",
    "        \n",
    "        test_predss = np.zeros(feat.shape[0])\n",
    "        \n",
    "        # prediction\n",
    "        for i in range(config[\"inference_n_splits\"]):\n",
    "            model_pipeline.predict(i, feat, infer=True)\n",
    "            \n",
    "            if config[\"model_mode\"] == \"stacking\":\n",
    "                model_pipeline.stacking(i, infer=True)\n",
    "                \n",
    "            test_predss += model_pipeline.inference_prediction / config[\"inference_n_splits\"]\n",
    "            \n",
    "        test_predss = zero_sum(test_predss, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(test_predss, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dda9c77e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936534Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-10T13:16:55.684169Z",
     "iopub.status.busy": "2023-12-10T13:16:55.683447Z",
     "iopub.status.idle": "2023-12-10T13:16:55.688291Z",
     "shell.execute_reply": "2023-12-10T13:16:55.687291Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.052707,
     "end_time": "2023-12-10T13:16:55.690489",
     "exception": false,
     "start_time": "2023-12-10T13:16:55.637782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single 1fold final / fianl no\n",
    "# single 1fold final / fianl\n",
    "# single 5fold final / fianl no\n",
    "# single 5fold final / fianl\n",
    "# stacking 1fold final / fianl no\n",
    "# stacking 1fold final / fianl\n",
    "# stacking 5fold final / fianl no\n",
    "# stacking 5fold final / fianl"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4106744,
     "sourceId": 7120258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4106761,
     "sourceId": 7120280,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4140979,
     "sourceId": 7167891,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yongmin-venv",
   "language": "python",
   "name": "yongmin-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 366.13633,
   "end_time": "2023-12-10T13:16:57.059116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-10T13:10:50.922786",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
