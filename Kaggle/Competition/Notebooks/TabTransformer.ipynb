{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f015b082-2a5b-4260-833b-430d7fa504fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 07:02:30.350392: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-10 07:02:30.483743: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 07:02:30.483802: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 07:02:30.484571: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 07:02:30.539045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/yongmin/yongmin-venv/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from itertools import combinations\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import gelu, softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "import random\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, ReLU, BatchNormalization, Activation, Concatenate\n",
    "from copy import deepcopy\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import joblib\n",
    "import functools\n",
    "\n",
    "import gc\n",
    "\n",
    "EPS = 1e-8\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47af01c1-9fc2-4dc2-bb2b-609b2f4f4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference, both\n",
    "KAGGLE_DATASET_NAME = \"tabtransformer-version-yongmin-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2da88e-bbfc-4e61-99c6-4f8c7fa5a716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4879e6-19c4-49ec-a37a-9845a8502eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"final_mode\": False,  # True : using final model, False : not using final model\n",
    "    \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "    'target': 'target',\n",
    "\n",
    "    'split_method': 'rolling',  # time_series, rolling, blocking, holdout\n",
    "    'n_splits': 3,  # number of splits\n",
    "    'correct': True,  # correct boundary\n",
    "    'gap': 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "\n",
    "    'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "    'train_test_ratio': 0.9,  # train, test ratio\n",
    "\n",
    "    'optuna_random_state': 42,\n",
    "}\n",
    "config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "    \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "    \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcec275-4f60-472a-bc1a-98d9ba4de03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      " 99%|███████████████████████████████████████▍| 198M/201M [00:06<00:00, 29.0MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:06<00:00, 31.2MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dca422-8cf3-4cdb-9ab4-d36bdf36035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470390be-3975-443f-a1dd-7b7a257fc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3b04b7-5573-4298-96ae-4099784d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self, data, infer=False):\n",
    "        self.data = memory_reduction.reduce_mem_usage(data)\n",
    "        self.infer = infer\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def handle_missing_data(self):\n",
    "        # 결측치 처리 코드\n",
    "        self.data = self.data.dropna(subset=[\"target\"]) if self.infer == False else self.data\n",
    "        self.data = self.data.fillna(0, columns = ['far_price', 'near_price'])\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def handle_outliers(self):\n",
    "        # 이상치 처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def normalize(self):\n",
    "        # 정규화 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def custom_preprocessing(self):\n",
    "        # 사용자 정의 전처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape}) \\n\\n {self.data}\")\n",
    "    def transform(self):\n",
    "        # 전처리 수행 코드 (위의 메소드 활용 가능)\n",
    "        self.handle_missing_data()\n",
    "        # self.handle_outliers()\n",
    "        # self.normalize()\n",
    "        # self.custom_preprocessing()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ff1fdf-7a5c-456f-bdfe-01b8594dc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084010b1-c76a-4b3a-b4de-a64406cfd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window_size=7):\n",
    "    price_diff = data['wap'].diff()\n",
    "    gain = price_diff.where(price_diff > 0, 0)\n",
    "    loss = -price_diff.where(price_diff < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window_size).mean()\n",
    "    avg_loss = loss.rolling(window=window_size).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a948267c-22f4-4559-9429-86a160a4666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b66307-65c5-4f58-bef4-fd2558144099",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "_weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e20cf08-4342-4c32-8ad1-af0cfd412a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_0(self, *args, version_name=\"feature_version_yongmin_0\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df['dow'] = self.data[\"date_id\"] % 5\n",
    "        df['seconds'] = self.data['seconds_in_bucket'] % 60\n",
    "        df['minute'] = self.data['seconds_in_bucket'] // 60\n",
    "    \n",
    "        df[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "        df['cum_wap'] = self.data.groupby(['stock_id'])['wap'].cumprod()\n",
    "    \n",
    "        for i in [1, 6]:\n",
    "            df[f'pct_change_{i}'] = self.data.groupby(['stock_id', 'seconds_in_bucket'])['wap'].pct_change(i).fillna(0)\n",
    "\n",
    "            f = lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "            df[f'polarize_pct_{i}'] = df[f'pct_change_{i}'].apply(f)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_1(self, *args, version_name=\"feature_version_yongmin_1\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        \n",
    "        window_size = 6\n",
    "        short_window = 1\n",
    "        long_window = 6\n",
    "\n",
    "        df['vol_std'] = self.data.groupby(['stock_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['rolling_vol_di'] = self.data.groupby(['date_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['std_st'] = self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['wap_pctch'] = self.data.groupby(['stock_id','date_id'])['wap'].pct_change().values*100\n",
    "        df['short_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=short_window, adjust=False).mean().values\n",
    "        df['long_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=long_window, adjust=False).mean().values\n",
    "        wap_mean = self.data['wap'].mean()\n",
    "        df['wap_vs_market'] = self.data['wap'] - self.data.groupby(['stock_id'])['wap'].transform('mean')\n",
    "        df['macd'] = df['short_ema'] - df['long_ema']\n",
    "        \n",
    "        # Bollinger Bands calculation within each stock, date, and time\n",
    "        df['bollinger_upper'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values + 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['bollinger_lower'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values - 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        # RSI calculation within each stock, date, and time\n",
    "        df['rsi'] = self.data.groupby(['stock_id']).apply(calculate_rsi).values\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_2(self, *args, version_name=\"feature_version_yongmin_2\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        self.data[\"stock_weights\"] = self.data[\"stock_id\"].map(_weights)\n",
    "        self.data[\"weighted_wap\"] = self.data[\"stock_weights\"] * self.data[\"wap\"]\n",
    "        df['wap_momentum'] = self.data.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "\n",
    "        df[\"imbalance_momentum\"] = self.data.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / self.data['matched_size']\n",
    "        self.data[\"price_spread\"] = self.data[\"ask_price\"] - self.data[\"bid_price\"]\n",
    "        \n",
    "        self.data[\"mid_price\"] = self.data.eval(\"(ask_price + bid_price) / 2\")\n",
    "        self.data[\"liquidity_imbalance\"] = self.data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n",
    "        df[\"matched_imbalance\"] = self.data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n",
    "        df[\"size_imbalance\"] = self.data.eval(f\"bid_size / ask_size+{EPS}\")\n",
    "        \n",
    "        df[\"spread_intensity\"] = self.data.groupby(['stock_id'])['price_spread'].diff()\n",
    "        df['price_pressure'] = self.data['imbalance_size'] * (self.data['ask_price'] - self.data['bid_price'])\n",
    "        df['market_urgency'] = self.data['price_spread'] * self.data['liquidity_imbalance']\n",
    "        df['depth_pressure'] = (self.data['ask_size'] - self.data['bid_size']) * (self.data['far_price'] - self.data['near_price'])\n",
    "        \n",
    "        df['spread_depth_ratio'] = (self.data['ask_price'] - self.data['bid_price']) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        df['mid_price_movement'] = self.data['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "        \n",
    "        df['micro_price'] = ((self.data['bid_price'] * self.data['ask_size']) + (self.data['ask_price'] * self.data['bid_size'])) / (self.data['bid_size'] + self.data['ask_size'])\n",
    "        df['relative_spread'] = (self.data['ask_price'] - self.data['bid_price']) / self.data['wap']\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_1(self, *args, version_name=\"feature_version_alvin_1\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = self.data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n",
    "\n",
    "        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "            triplet_feature = calculate_triplet_imbalance_numba(c, self.data)\n",
    "            df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_market_cap(self, *args, version_name=\"feature_market_cap\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Market Cap\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_sector(self, *args, version_name=\"feature_sector\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Sector\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_industry(self, *args, version_name=\"feature_industry\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df = get_stock_info(df, self.data, \"Industry\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        final_data = concat_df\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873ff665-9e63-456b-880d-9f2a106d42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \"\"\"\n",
    "    데이터 분리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        데이터 분리 방식\n",
    "    n_splits : int\n",
    "        데이터 분리 개수\n",
    "    correct : bool\n",
    "        데이터 분리 시 boundary를 맞출지 여부\n",
    "    initial_fold_size_ratio : float\n",
    "        초기 fold size 비율\n",
    "    train_test_ratio : float\n",
    "        train, test 비율\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    split()\n",
    "        데이터 분리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n",
    "                 overlap=True, train_start=0,\n",
    "                 train_end=390, valid_start=391, valid_end=480):\n",
    "        self.method = method\n",
    "        self.n_splits = n_splits\n",
    "        self.correct = correct\n",
    "        self.initial_fold_size_ratio = initial_fold_size_ratio\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "\n",
    "        self.gap = gap\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # only for holdout method\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.valid_start = valid_start\n",
    "        self.valid_end = valid_end\n",
    "\n",
    "        self.target = config[\"target\"]\n",
    "\n",
    "        self.boundaries = []\n",
    "\n",
    "    def split(self, data):\n",
    "        self.data = data #reduce_mem_usage(data)\n",
    "        self.all_dates = self.data['date_id_copy'].unique()\n",
    "        if self.method == \"time_series\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Time series split method only works with n_splits > 1\")\n",
    "            return self._time_series_split(data)\n",
    "        elif self.method == \"rolling\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"blocking\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n",
    "            self.initial_fold_size_ratio = 1.0 / self.n_splits\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"holdout\":\n",
    "            if self.n_splits != 1:\n",
    "                raise ValueError(\"Holdout method only works with n_splits=1\")\n",
    "            return self._holdout_split(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "    def _correct_boundary(self, data, idx, direction=\"forward\"):\n",
    "        # Correct the boundary based on date_id_copy\n",
    "        original_idx = idx\n",
    "        if idx == 0 or idx == len(data) - 1:\n",
    "            return idx\n",
    "        if direction == \"forward\":\n",
    "            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx += 1\n",
    "        elif direction == \"backward\":\n",
    "            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx -= 1\n",
    "            idx += 1  # adjust to include the boundary\n",
    "        return idx\n",
    "\n",
    "    def _time_series_split(self, data):\n",
    "        n = len(data)\n",
    "        initial_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n",
    "        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n",
    "            train_size = fold_size - initial_test_size\n",
    "\n",
    "            if self.correct:\n",
    "                train_size = self._correct_boundary(data, train_size, \"forward\")\n",
    "                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n",
    "            else:\n",
    "                end_of_test = train_size + initial_test_size\n",
    "\n",
    "            train_slice = data.iloc[:train_size]\n",
    "            test_slice = data.iloc[train_size:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "\n",
    "    def _rolling_split(self, data):\n",
    "        n = len(data)\n",
    "        total_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n",
    "        gap_size = int(total_fold_size * self.gap)\n",
    "        train_size = total_fold_size - test_size\n",
    "        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n",
    "\n",
    "        end_of_test = n - 1\n",
    "        start_of_test = end_of_test - test_size\n",
    "        end_of_train = start_of_test - gap_size\n",
    "        start_of_train = end_of_train - train_size\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            if self.correct:\n",
    "                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n",
    "                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n",
    "                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n",
    "                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n",
    "\n",
    "            train_slice = data[start_of_train:end_of_train]\n",
    "            test_slice = data[start_of_test:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[0],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "            start_of_train = max(start_of_train - rolling_increment, 0)\n",
    "            end_of_train -= rolling_increment\n",
    "            start_of_test -= rolling_increment\n",
    "            end_of_test -= rolling_increment\n",
    "\n",
    "    def _holdout_split(self, data):\n",
    "        # train_start ~ train_end : 학습 데이터 기간\n",
    "        # valid_start ~ valid_end : 검증 데이터 기간\n",
    "        # 학습 및 검증 데이터 분리\n",
    "        threshold = int(data['date_id_copy'].nunique() * self.train_test_ratio)\n",
    "        self.train_start, self.train_end = 0, data['date_id_copy'].unique()[threshold]\n",
    "        self.valid_start, self.valid_end = self.train_end + self.gap, data['date_id_copy'].unique()[-1]\n",
    "        \n",
    "        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n",
    "        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n",
    "\n",
    "        train_slice = data[train_mask]\n",
    "        valid_slice = data[valid_mask]\n",
    "\n",
    "        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_train = train_slice[self.target]\n",
    "        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_valid = valid_slice[self.target]\n",
    "\n",
    "        self.boundaries.append((\n",
    "            train_slice['date_id_copy'].iloc[0],\n",
    "            train_slice['date_id_copy'].iloc[-1],\n",
    "            valid_slice['date_id_copy'].iloc[0],\n",
    "            valid_slice['date_id_copy'].iloc[-1]\n",
    "        ))\n",
    "        yield X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    def visualize_splits(self):\n",
    "        print(\"Visualizing Train/Test Split Boundaries\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n",
    "            train_width = train_end - train_start + 1\n",
    "            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n",
    "            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n",
    "                     color='black', fontsize=8)\n",
    "\n",
    "            test_width = test_end - test_start + 1\n",
    "            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n",
    "            if test_width > 0:\n",
    "                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n",
    "                         color='black', fontsize=8)\n",
    "\n",
    "        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n",
    "        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n",
    "        plt.xlabel(\"date_id_copy\")\n",
    "        plt.title(\"Train/Test Split Boundaries\")\n",
    "        plt.grid(axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f308e66b-005a-4b28-b430-1a7e7f7697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import gelu, softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # parametreleri\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # batch-layer\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TabTransformer(keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "            categories,\n",
    "            num_continuous,\n",
    "            dim,\n",
    "            dim_out,\n",
    "            depth,\n",
    "            heads,\n",
    "            attn_dropout,\n",
    "            ff_dropout,\n",
    "            mlp_hidden,\n",
    "            normalize_continuous = True):\n",
    "        \n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        # --> continuous inputs\n",
    "        self.normalize_continuous = normalize_continuous\n",
    "        if normalize_continuous:\n",
    "            self.continuous_normalization = layers.LayerNormalization()\n",
    "\n",
    "        # --> categorical inputs\n",
    "\n",
    "        # embedding\n",
    "        self.embedding_layers = []\n",
    "        for number_of_classes in categories:\n",
    "            self.embedding_layers.append(layers.Embedding(input_dim = number_of_classes, output_dim = dim))\n",
    "\n",
    "        # concatenation\n",
    "        self.embedded_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # adding transformers\n",
    "        self.transformers = []\n",
    "        for _ in range(depth):\n",
    "            self.transformers.append(TransformerBlock(dim, heads, dim))\n",
    "        self.flatten_transformer_output = layers.Flatten()\n",
    "\n",
    "        # --> MLP\n",
    "        self.pre_mlp_concatenation = layers.Concatenate()\n",
    "\n",
    "        # mlp layers\n",
    "        self.mlp_layers = []\n",
    "        for size, activation in mlp_hidden:\n",
    "            self.mlp_layers.append(layers.Dense(size, activation=activation))\n",
    "\n",
    "        self.output_layer = layers.Dense(dim_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        categorical_inputs = inputs\n",
    "#         print(inputs[:,0])\n",
    "        # --> categorical\n",
    "        embedding_outputs = []\n",
    "        for i in range(len(self.embedding_layers)):\n",
    "            embedding_outputs.append(tf.expand_dims(self.embedding_layers[i](categorical_inputs[:,i]),axis=1))\n",
    "#         print(embedding_outputs[0].shape)\n",
    "        categorical_inputs = self.embedded_concatenation(embedding_outputs)\n",
    "#         categorical_inputs = tf.expand_dims(categorical_inputs,axis=1)\n",
    "        for transformer in self.transformers:\n",
    "            categorical_inputs = transformer(categorical_inputs)\n",
    "        contextual_embedding = self.flatten_transformer_output(categorical_inputs)\n",
    "\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            mlp_input = mlp_layer(contextual_embedding)\n",
    "\n",
    "        return self.output_layer(mlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "819bc6dd-97ac-4226-b8c1-d6981e43febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d60b5dd-2e61-4845-9139-57ab135ba64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.61 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_0, Elapsed time: 2.35 seconds, shape((5237980, 9))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_1, Elapsed time: 10.35 seconds, shape((5237980, 11))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_2, Elapsed time: 1.37 seconds, shape((5237980, 12))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_alvin_1, Elapsed time: 1.37 seconds, shape((5237980, 23))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 15.43 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 0.50 seconds, shape((5237980, 75))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 17.15 seconds, shape((5237980, 76))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 07:04:14.759023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.769828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.769911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.770876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.770948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.770995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.815069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.815131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.815174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 07:04:14.815209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5138980, 74) (5138980,) (55000, 74) (55000,)\n",
      "Fitting Model - No CV\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_transformer/dense_12/kernel:0', 'tab_transformer/dense_12/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_transformer/dense_12/kernel:0', 'tab_transformer/dense_12/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_transformer/dense_12/kernel:0', 'tab_transformer/dense_12/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_transformer/dense_12/kernel:0', 'tab_transformer/dense_12/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 07:04:21.249725: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa3dbbb8b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 07:04:21.249738: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-12-10 07:04:21.252945: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-10 07:04:21.263366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-12-10 07:04:21.305975: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80296/80296 [==============================] - 978s 12ms/step - loss: 6.4223 - mae: 6.4223 - val_loss: 5.2831 - val_mae: 5.2831 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "80296/80296 [==============================] - 951s 12ms/step - loss: 6.4225 - mae: 6.4225 - val_loss: 5.2829 - val_mae: 5.2829 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "80296/80296 [==============================] - 965s 12ms/step - loss: 6.4223 - mae: 6.4223 - val_loss: 5.2831 - val_mae: 5.2831 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "80296/80296 [==============================] - 995s 12ms/step - loss: 6.4222 - mae: 6.4222 - val_loss: 5.2845 - val_mae: 5.2845 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "80296/80296 [==============================] - 969s 12ms/step - loss: 6.4223 - mae: 6.4223 - val_loss: 5.2844 - val_mae: 5.2844 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "80296/80296 [==============================] - 969s 12ms/step - loss: 6.4221 - mae: 6.4221 - val_loss: 5.2839 - val_mae: 5.2839 - lr: 5.0000e-04\n",
      "Epoch 7/30\n",
      "80296/80296 [==============================] - 985s 12ms/step - loss: 6.4221 - mae: 6.4221 - val_loss: 5.2839 - val_mae: 5.2839 - lr: 5.0000e-04\n",
      "Save Error\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(ckp_path)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain NN Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_absolute_error(y_train, pred))\n\u001b[1;32m     85\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "if config[\"train_mode\"]:\n",
    "    # Read Data\n",
    "    df = (pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "          .drop(['time_id', 'row_id'], axis=1))\n",
    "\n",
    "    # Feature Engineering\n",
    "\n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', 'feature_version_yongmin_2',\n",
    "                                                             'feature_version_alvin_1'],\n",
    "                                       dependencies=dependencies)\n",
    "    \n",
    "    feature_engineer.generate_global_features(df)\n",
    "    \n",
    "    df = feature_engineer.transform()\n",
    "    \n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # Set Variables Type\n",
    "    categorical = ['stock_id']\n",
    "\n",
    "    continuous = list(set(df.columns) - set(categorical) - set(['target', 'date_id_copy']))\n",
    "\n",
    "    rubbish = gc.collect()\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    LR = 0.001\n",
    "    EPOCH = 30\n",
    "    INPUT_SHAPE = df.shape[-1] - 2 # -2 for target and date_id_copy\n",
    "    # nu = [df[col].nunique() for col in df.drop(['target', 'date_id_copy'], axis=1).columns]\n",
    "    nu = [df[col].nunique() for col in categorical]\n",
    "\n",
    "    ckp_path = os.path.join(config['model_dir'], 'tabtransformer_30epochV0.h5')\n",
    "    if not os.path.exists(config['model_dir']):\n",
    "        os.mkdir(config['model_dir'])\n",
    "    \n",
    "    model = TabTransformer(\n",
    "                            categories = nu, # number of unique elements in each categorical feature\n",
    "                            num_continuous = len(continuous),      # number of numerical features\n",
    "                            dim = 16,                # embedding/transformer dimension\n",
    "                            dim_out = 1,            # dimension of the model output\n",
    "                            depth = 6,               # number of transformer layers in the stack\n",
    "                            heads = 8,               # number of attention heads\n",
    "                            attn_dropout = 0.2,      # attention layer dropout in transformers\n",
    "                            ff_dropout = 0.2,        # feed-forward layer dropout in transformers\n",
    "                            mlp_hidden = [(128, 'relu'), (64, 'relu')] # mlp layer dimensions and activations\n",
    "                        )\n",
    "    \n",
    "    model.compile(AdamW(LR), 'mae', metrics=['mae'])\n",
    "\n",
    "    splitter = Splitter(method='holdout', n_splits=1, correct=True, train_test_ratio=0.98, gap=5)\n",
    "    for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(df)):\n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "        \n",
    "        trainds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        trainds_batch = trainds.batch(BATCH_SIZE, drop_remainder = True)\n",
    "        \n",
    "        valds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        valds_batch = valds.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "        rlr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=3, verbose=0, min_delta=1e-5, mode='min')\n",
    "        ckp = ModelCheckpoint(ckp_path, monitor='val_mae', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_mae', min_delta=1e-4, patience=5, mode='min', restore_best_weights=True, verbose=0)\n",
    "    \n",
    "        model_callbacks = [rlr, ckp, es]\n",
    "    \n",
    "        print(f\"Fitting Model - No CV\")\n",
    "        model.fit(trainds_batch,\n",
    "                  validation_data = valds_batch,\n",
    "                  batch_size = BATCH_SIZE,\n",
    "                  epochs = EPOCH,\n",
    "                  callbacks = model_callbacks)\n",
    "        \n",
    "        model.save_weights(ckp_path)\n",
    "        print(\"Save Error\")\n",
    "    \n",
    "        pred = model.predict(X_train, batch_size=batch_size).ravel()\n",
    "        print(\"Train NN Score:\", mean_absolute_error(y_train, pred))\n",
    "    \n",
    "        K.clear_session()\n",
    "        del model\n",
    "        rubbish = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34e084f8-1304-44ba-8071-9a5fbd33d423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ./models/20231210_16:02:31/dataset-metadata.json\n",
      "Starting upload for file tabtransformer_30epochV0.h5\n",
      "100%|█████████████████████████████████████████| 376k/376k [00:02<00:00, 151kB/s]\n",
      "Upload successful: tabtransformer_30epochV0.h5 (376KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/jhk3211/tabtransformer-version-yongmin-0\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96244c8a-598d-4437-9c45-18f8abde2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "#       .drop(['time_id', 'row_id'], axis=1))\n",
    "\n",
    "# # Feature Engineering\n",
    "\n",
    "# feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1'],\n",
    "#                                    dependencies=dependencies)\n",
    "\n",
    "# feature_engineer.generate_global_features(df)\n",
    "\n",
    "# df = feature_engineer.transform()\n",
    "\n",
    "# # fillna\n",
    "# df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "#       .fillna(method='ffill')\n",
    "#       .fillna(0)\n",
    "#      )\n",
    "\n",
    "# df = reduce_mem_usage(df)\n",
    "# rubbish = gc.collect()\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "# LR = 0.001\n",
    "# EPOCH = 30\n",
    "# INPUT_SHAPE = df.shape[-1] - 2 # -2 for target and date_id_copy\n",
    "\n",
    "# final_model = Sequential([\n",
    "#                     Input(shape = (INPUT_SHAPE), batch_size=BATCH_SIZE),\n",
    "#                     SAINT(3, 6, 8, 4, 4),\n",
    "#                     Dense(1 , activation = 'linear')\n",
    "# ])\n",
    "\n",
    "# final_model.compile(tf.keras.optimizers.Adam(LR), 'mae', metrics=['mae'])\n",
    "\n",
    "# model_path = \"./models/20231130_11:05:26 /saint_30epoch.h5\"\n",
    "# final_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77858e-26fe-4f7a-88ad-a328a49b04c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yongmin-venv",
   "language": "python",
   "name": "yongmin-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
