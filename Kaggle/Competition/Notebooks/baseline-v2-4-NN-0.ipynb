{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5596e2",
   "metadata": {
    "papermill": {
     "duration": 0.013174,
     "end_time": "2023-11-09T03:15:46.560482",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.547308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 사용 설명서\n",
    "\n",
    "## 0. requirements\n",
    "#### 0.1. 로컬 환경에서 package 설치법\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>```\n",
    "\n",
    "#### 0.2. kaggle api 설치법\n",
    "- https://www.kaggle.com/docs/api#getting-started-installation-&-authentication 참고\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle```\n",
    "- kaggle api token 다운로드 후 upload (kaggle.json)\n",
    "- ```!mkdir ~/.kaggle``` \n",
    "- ```!mv ~/kaggle.json ~/.kaggle/``` (kaggle.json 파일을 ~/.kaggle/ 로 이동)\n",
    "- ```!chmod 600 ~/.kaggle/kaggle.json``` (권한 설정)\n",
    "\n",
    "## 1. config 설정\n",
    "\n",
    "#### 1.1. init config\n",
    "- MODE: train, inference 중 선택 (train : 로컬 환경, inference : 캐글 환경)\n",
    "- KAGGLE_DATASET_NAME: 캐글 환경에서 inference 시 사용할 데이터셋 이름 \n",
    "  - 이 이름으로 캐글 데이터셋이 생성됩니다. (중복 불가)\n",
    "\n",
    "#### 1.2. train / inference config\n",
    "- model_directory: 모델 저장 경로\n",
    "- data_directory: 데이터 경로\n",
    "- train_mode: train 모드 여부\n",
    "- infer_mode: inference 모드 여부\n",
    "\n",
    "#### 1.3. model config\n",
    "- model_name: 사용할 모델 이름\n",
    "    - 실제 아래 models_config에 있는 모델 이름과 동일해야 합니다 (아래중에서 선택하는것임).(:list)\n",
    "- target: target column 이름\n",
    "- split_method: 데이터 분리 방식\n",
    "  - time_series: 시계열 데이터 분리\n",
    "  - rolling: 롤링 윈도우 방식 데이터 분리\n",
    "  - blocking: 블록 방식 데이터 분리\n",
    "  - holdout: holdout 방식 데이터 분리\n",
    "- n_splits: 데이터 분리 개수 (1 ~)\n",
    "- correct: 데이터 분리 시 날짜 boundary를 맞출지 여부 (True / False)\n",
    "- initial_fold_size_ratio: 초기 fold size 비율 (0 ~ 1)\n",
    "- train_test_ratio: train, test 비율 (0 ~ 1)\n",
    "- ~train_start: 학습 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~train_end: 학습 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_start: 검증 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_end: 검증 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- optuna_random_state: optuna random state\n",
    "                                - \n",
    "#### 1.4. model heyperparameter config\n",
    "- models_config: 모델 하이퍼파라미터 설정\n",
    "    - model: 모델 클래스\n",
    "    - params: 모델 하이퍼파라미터들\n",
    "        - ... : 모델 하이퍼파라미터\n",
    "\n",
    "## 2. Global Method\n",
    "- reduce_mem_usage: 메모리 사용량 줄이는 함수\n",
    "- compute_triplet_imbalance: triplet imbalance 계산 함수\n",
    "- calculate_triplet_imbalance_numba: triplet imbalance 계산 함수\n",
    "- print_log: 함수 실행 전후에 원하는 코드를 실행해주는 decorator 함수입니다.\n",
    "- zero_sum: zero sum 함수\n",
    "\n",
    "## 3. Pre Code\n",
    "- DataPreprocessor: 데이터 전처리 클래스\n",
    "- FeatureEngineer: 피쳐 엔지니어링 클래스\n",
    "- Splitter: 데이터 분리 클래스\n",
    "- Model: 모델 클래스\n",
    "- Trainer: 학습 클래스\n",
    "\n",
    "## 4. Main Code\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cde1e6",
   "metadata": {
    "papermill": {
     "duration": 0.013176,
     "end_time": "2023-11-09T03:15:46.586959",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.573783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5c752",
   "metadata": {
    "papermill": {
     "duration": 0.013293,
     "end_time": "2023-11-09T03:15:46.613651",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.600358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830e3ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276368Z",
     "start_time": "2023-11-18T04:44:53.963992Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:39:35.914126Z",
     "iopub.status.busy": "2023-11-10T07:39:35.913770Z",
     "iopub.status.idle": "2023-11-10T07:39:35.917883Z",
     "shell.execute_reply": "2023-11-10T07:39:35.917021Z",
     "shell.execute_reply.started": "2023-11-10T07:39:35.914096Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021933,
     "end_time": "2023-11-09T03:15:46.649226",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.627293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204d298",
   "metadata": {
    "papermill": {
     "duration": 0.014012,
     "end_time": "2023-11-09T03:15:46.677485",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.663473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 0.2. kaggle api 설치법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a581ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276461Z",
     "start_time": "2023-11-18T04:44:53.998348Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:39:35.920097Z",
     "iopub.status.busy": "2023-11-10T07:39:35.919759Z",
     "iopub.status.idle": "2023-11-10T07:39:35.927271Z",
     "shell.execute_reply": "2023-11-10T07:39:35.926541Z",
     "shell.execute_reply.started": "2023-11-10T07:39:35.920067Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021246,
     "end_time": "2023-11-09T03:15:46.712389",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.691143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f541b034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276579Z",
     "start_time": "2023-11-18T04:44:54.001642Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:39:35.928581Z",
     "iopub.status.busy": "2023-11-10T07:39:35.928269Z",
     "iopub.status.idle": "2023-11-10T07:39:35.935981Z",
     "shell.execute_reply": "2023-11-10T07:39:35.935184Z",
     "shell.execute_reply.started": "2023-11-10T07:39:35.928549Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020857,
     "end_time": "2023-11-09T03:15:46.747089",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.726232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf90e044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276667Z",
     "start_time": "2023-11-18T04:44:54.001730Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:39:35.937809Z",
     "iopub.status.busy": "2023-11-10T07:39:35.937497Z",
     "iopub.status.idle": "2023-11-10T07:39:35.946576Z",
     "shell.execute_reply": "2023-11-10T07:39:35.945846Z",
     "shell.execute_reply.started": "2023-11-10T07:39:35.937751Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02085,
     "end_time": "2023-11-09T03:15:46.782033",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.761183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mv ~/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39c499a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276757Z",
     "start_time": "2023-11-18T04:44:54.001870Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:39:36.002552Z",
     "iopub.status.busy": "2023-11-10T07:39:36.002295Z",
     "iopub.status.idle": "2023-11-10T07:39:36.006357Z",
     "shell.execute_reply": "2023-11-10T07:39:36.005510Z",
     "shell.execute_reply.started": "2023-11-10T07:39:36.002530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021282,
     "end_time": "2023-11-09T03:15:46.817302",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.796020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee8387",
   "metadata": {
    "papermill": {
     "duration": 0.013582,
     "end_time": "2023-11-09T03:15:46.844948",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.831366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. config 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9e664",
   "metadata": {
    "papermill": {
     "duration": 0.013741,
     "end_time": "2023-11-09T03:15:46.924523",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.910782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1. init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7508e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:54.276841Z",
     "start_time": "2023-11-18T04:44:54.045651Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026122,
     "end_time": "2023-11-09T03:15:46.964537",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.938415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference, both\n",
    "KAGGLE_DATASET_NAME = \"model-nn-version-yongmin-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7158266d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.081563Z",
     "start_time": "2023-11-18T04:44:54.045860Z"
    },
    "papermill": {
     "duration": 5.644978,
     "end_time": "2023-11-09T03:16:25.965358",
     "exception": false,
     "start_time": "2023-11-09T03:16:20.320380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 10:09:23.943804: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 10:09:23.963294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 10:09:23.963314: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 10:09:23.963327: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 10:09:23.967282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import functools\n",
    "import time\n",
    "from numba import njit, prange\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08a77d",
   "metadata": {
    "papermill": {
     "duration": 0.014983,
     "end_time": "2023-11-09T03:16:25.995747",
     "exception": false,
     "start_time": "2023-11-09T03:16:25.980764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.2. train / inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ba0759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.084772Z",
     "start_time": "2023-11-18T04:44:55.081326Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.3.2', '2.0.1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__, xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b79898451c0562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.084927Z",
     "start_time": "2023-11-18T04:44:55.081831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "EPS = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb204dfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.174297Z",
     "start_time": "2023-11-18T04:44:55.082342Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025998,
     "end_time": "2023-11-09T03:16:26.037324",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.011326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d287c",
   "metadata": {
    "papermill": {
     "duration": 0.01504,
     "end_time": "2023-11-09T03:16:26.067759",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.052719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.3. model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e997cff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:55.178131Z",
     "start_time": "2023-11-18T04:44:55.160976Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026838,
     "end_time": "2023-11-09T03:16:26.109734",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.082896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"final_mode\": False,  # True : using final model, False : not using final model\n",
    "    \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "    'target': 'target',\n",
    "\n",
    "    'split_method': 'rolling',  # time_series, rolling, blocking, holdout\n",
    "    'n_splits': 3,  # number of splits\n",
    "    'correct': True,  # correct boundary\n",
    "    'gap': 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "\n",
    "    'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "    'train_test_ratio': 0.9,  # train, test ratio\n",
    "\n",
    "    'optuna_random_state': 42,\n",
    "}\n",
    "config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "    \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "    \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b634c97",
   "metadata": {
    "papermill": {
     "duration": 0.015075,
     "end_time": "2023-11-09T03:16:26.139996",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.124921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.4. model heyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c05d4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.582Z",
     "start_time": "2023-11-18T04:44:55.161493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      "100%|███████████████████████████████████████▉| 200M/201M [00:07<00:00, 33.1MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:07<00:00, 29.3MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d6f57",
   "metadata": {
    "papermill": {
     "duration": 0.015155,
     "end_time": "2023-11-09T03:16:26.253174",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.238019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Global Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f89921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.598960Z",
     "start_time": "2023-11-18T04:45:03.585612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030413,
     "end_time": "2023-11-09T03:16:26.299001",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.268588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc4af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644285Z",
     "start_time": "2023-11-18T04:45:03.620103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035964,
     "end_time": "2023-11-09T03:16:26.484162",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.448198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd5cb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.644943Z",
     "start_time": "2023-11-18T04:45:03.620276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0316,
     "end_time": "2023-11-09T03:16:26.537507",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.505907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f970b7",
   "metadata": {
    "papermill": {
     "duration": 0.017946,
     "end_time": "2023-11-09T03:16:26.571351",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.553405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 각 클래스의 method는 각자 필요에 따라 추가 해서 사용하면 됩니다. 이때 class의 주석에 method를 추가하고, method의 주석에는 method의 역할을 간단하게 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4d608",
   "metadata": {
    "papermill": {
     "duration": 0.019169,
     "end_time": "2023-11-09T03:16:26.609856",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.590687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Pre Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142b73e",
   "metadata": {
    "papermill": {
     "duration": 0.019027,
     "end_time": "2023-11-09T03:16:26.648726",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.629699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4829f9",
   "metadata": {
    "papermill": {
     "duration": 0.018425,
     "end_time": "2023-11-09T03:16:26.733619",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.715194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26abd52da06b46ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.863864Z",
     "start_time": "2023-11-18T04:45:03.863395Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a669f5a170c69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.937718Z",
     "start_time": "2023-11-18T04:45:03.863730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    이 클래스는 데이터 세트에 대한 피처 엔지니어링을 수행합니다.\n",
    "    클래스의 주요 목적은 데이터 세트에 대한 다양한 변환 및 가공을 통해 머신 러닝 모델에 적합한 형태의 피처를 생성하는 것입니다.\n",
    "\n",
    "    클래스에는 다음과 같은 메서드들이 포함됩니다:\n",
    "    1. feature_version_n: 피처 엔지니어링의 다양한 버전을 구현합니다. \n",
    "       이 메서드들은 데이터에 대한 고유한 변환을 적용하며, 다른 피처 엔지니어링 버전의 결과를 결합할 수도 있습니다.\n",
    "    2. transform: 모든 피처 엔지니어링 버전의 결과를 결합하여 최종적으로 통합된 데이터 세트를 생성하고 반환합니다.\n",
    "\n",
    "    feature_version_n 메서드의 'args' 매개변수에 대한 설명:\n",
    "    - 'args'는 가변 인자로, 다른 피처 엔지니어링 버전의 결과를 전달하는 데 사용됩니다.\n",
    "    - 예를 들어, feature_version_2 메서드가 feature_version_0의 결과를 필요로 하는 경우, \n",
    "      feature_version_2(feature_version_0()) 형태로 호출할 수 있습니다.\n",
    "    - 이런 방식으로 'args'를 사용하면, 하나의 피처 엔지니어링 버전이 다른 버전의 결과를 참조하고 활용할 수 있습니다.\n",
    "\n",
    "    주의: 이 클래스는 원본 데이터를 직접 수정하지 않습니다. 모든 변환은 새로운 데이터 프레임에 적용되며, \n",
    "    transform 메서드는 최종적으로 통합된 데이터 세트를 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    def _save_to_parquet(self, df, version_name):\n",
    "        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n",
    "        df.to_parquet(file_path)\n",
    "        print(f\"Saved {version_name} to {file_path}\")\n",
    "\n",
    "    def _load_from_parquet(self, version_name):\n",
    "        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n",
    "        if os.path.exists(file_path):\n",
    "            return pq.read_table(file_path).to_pandas()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {file_path} not found.\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_0(self, *args, version_name=\"feature_version_yongmin_0\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df['dow'] = self.data[\"date_id\"] % 5\n",
    "        df['seconds'] = self.data['seconds_in_bucket'] % 60\n",
    "        df['minute'] = self.data['seconds_in_bucket'] // 60\n",
    "    \n",
    "        df[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "    \n",
    "        for i in [1, 5, 10]:\n",
    "            df[f'pct_change_{i}'] = self.data.groupby(['stock_id', 'seconds_in_bucket'])['wap'].pct_change(i).fillna(0)\n",
    "    \n",
    "        return df\n",
    "        \n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324dac9",
   "metadata": {
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-11-09T03:16:26.835377",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.819592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f8a19be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:03.939923Z",
     "start_time": "2023-11-18T04:45:03.937632Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.060516,
     "end_time": "2023-11-09T03:16:26.914353",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.853837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \"\"\"\n",
    "    데이터 분리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        데이터 분리 방식\n",
    "    n_splits : int\n",
    "        데이터 분리 개수\n",
    "    correct : bool\n",
    "        데이터 분리 시 boundary를 맞출지 여부\n",
    "    initial_fold_size_ratio : float\n",
    "        초기 fold size 비율\n",
    "    train_test_ratio : float\n",
    "        train, test 비율\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    split()\n",
    "        데이터 분리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n",
    "                 overlap=True, train_start=0,\n",
    "                 train_end=390, valid_start=391, valid_end=480):\n",
    "        self.method = method\n",
    "        self.n_splits = n_splits\n",
    "        self.correct = correct\n",
    "        self.initial_fold_size_ratio = initial_fold_size_ratio\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "\n",
    "        self.gap = gap\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # only for holdout method\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.valid_start = valid_start\n",
    "        self.valid_end = valid_end\n",
    "\n",
    "        self.target = config[\"target\"]\n",
    "\n",
    "        self.boundaries = []\n",
    "\n",
    "    def split(self, data):\n",
    "        self.data = reduce_mem_usage(data)\n",
    "        self.all_dates = self.data['date_id_copy'].unique()\n",
    "        if self.method == \"time_series\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Time series split method only works with n_splits > 1\")\n",
    "            return self._time_series_split(data)\n",
    "        elif self.method == \"rolling\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"blocking\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n",
    "            self.initial_fold_size_ratio = 1.0 / self.n_splits\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"holdout\":\n",
    "            if self.n_splits != 1:\n",
    "                raise ValueError(\"Holdout method only works with n_splits=1\")\n",
    "            return self._holdout_split(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "    def _correct_boundary(self, data, idx, direction=\"forward\"):\n",
    "        # Correct the boundary based on date_id_copy\n",
    "        original_idx = idx\n",
    "        if idx == 0 or idx == len(data) - 1:\n",
    "            return idx\n",
    "        if direction == \"forward\":\n",
    "            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx += 1\n",
    "        elif direction == \"backward\":\n",
    "            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx -= 1\n",
    "            idx += 1  # adjust to include the boundary\n",
    "        return idx\n",
    "\n",
    "    def _time_series_split(self, data):\n",
    "        n = len(data)\n",
    "        initial_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n",
    "        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n",
    "            train_size = fold_size - initial_test_size\n",
    "\n",
    "            if self.correct:\n",
    "                train_size = self._correct_boundary(data, train_size, \"forward\")\n",
    "                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n",
    "            else:\n",
    "                end_of_test = train_size + initial_test_size\n",
    "\n",
    "            train_slice = data.iloc[:train_size]\n",
    "            test_slice = data.iloc[train_size:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "\n",
    "    def _rolling_split(self, data):\n",
    "        n = len(data)\n",
    "        total_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n",
    "        gap_size = int(total_fold_size * self.gap)\n",
    "        train_size = total_fold_size - test_size\n",
    "        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n",
    "\n",
    "        end_of_test = n - 1\n",
    "        start_of_test = end_of_test - test_size\n",
    "        end_of_train = start_of_test - gap_size\n",
    "        start_of_train = end_of_train - train_size\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            if self.correct:\n",
    "                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n",
    "                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n",
    "                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n",
    "                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n",
    "\n",
    "            train_slice = data[start_of_train:end_of_train]\n",
    "            test_slice = data[start_of_test:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[0],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "            start_of_train = max(start_of_train - rolling_increment, 0)\n",
    "            end_of_train -= rolling_increment\n",
    "            start_of_test -= rolling_increment\n",
    "            end_of_test -= rolling_increment\n",
    "\n",
    "    def _holdout_split(self, data):\n",
    "        # train_start ~ train_end : 학습 데이터 기간\n",
    "        # valid_start ~ valid_end : 검증 데이터 기간\n",
    "        # 학습 및 검증 데이터 분리\n",
    "        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n",
    "        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n",
    "\n",
    "        train_slice = data[train_mask]\n",
    "        valid_slice = data[valid_mask]\n",
    "\n",
    "        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_train = train_slice[self.target]\n",
    "        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_valid = valid_slice[self.target]\n",
    "\n",
    "        self.boundaries.append((\n",
    "            train_slice['date_id_copy'].iloc[0],\n",
    "            train_slice['date_id_copy'].iloc[-1],\n",
    "            valid_slice['date_id_copy'].iloc[0],\n",
    "            valid_slice['date_id_copy'].iloc[-1]\n",
    "        ))\n",
    "        yield X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    def visualize_splits(self):\n",
    "        print(\"Visualizing Train/Test Split Boundaries\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n",
    "            train_width = train_end - train_start + 1\n",
    "            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n",
    "            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n",
    "                     color='black', fontsize=8)\n",
    "\n",
    "            test_width = test_end - test_start + 1\n",
    "            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n",
    "            if test_width > 0:\n",
    "                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n",
    "                         color='black', fontsize=8)\n",
    "\n",
    "        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n",
    "        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n",
    "        plt.xlabel(\"date_id_copy\")\n",
    "        plt.title(\"Train/Test Split Boundaries\")\n",
    "        plt.grid(axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a3a35",
   "metadata": {
    "papermill": {
     "duration": 0.017633,
     "end_time": "2023-11-09T03:16:26.985144",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.967511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba303af-2bce-4bea-8baa-05dd70a1bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(num_continuous_features, num_categorical_features, embedding_dims, num_labels, hidden_units, dropout_rates, learning_rate,l2_strength=0.01):\n",
    "    \n",
    "    # Numerical variables input\n",
    "    input_continuous = tf.keras.layers.Input(shape=(num_continuous_features,))\n",
    "    \n",
    "    # Categorical variables input\n",
    "    input_categorical = [tf.keras.layers.Input(shape=(1,)) \n",
    "                         for _ in range(len(num_categorical_features))]\n",
    "    \n",
    "    # Embedding layer for categorical variables\n",
    "    embeddings = [tf.keras.layers.Embedding(input_dim=num_categorical_features[i], \n",
    "                                            output_dim=embedding_dims[i], \n",
    "                                            embeddings_initializer='he_normal')(input_cat) \n",
    "                  for i, input_cat in enumerate(input_categorical)]\n",
    "    flat_embeddings = [tf.keras.layers.Flatten()(embed) for embed in embeddings]\n",
    "    \n",
    "    # concat numerical and categorical\n",
    "    concat_input = tf.keras.layers.concatenate([input_continuous] + flat_embeddings)\n",
    "    \n",
    "    # MLP\n",
    "    x = tf.keras.layers.BatchNormalization()(concat_input)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    \n",
    "    for i in range(len(hidden_units)): \n",
    "        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        #x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n",
    "        \n",
    "    #No activation\n",
    "    out = tf.keras.layers.Dense(num_labels, kernel_initializer='he_normal')(x) \n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_continuous] + input_categorical, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_absolute_error',  \n",
    "                  metrics=['mean_absolute_error'])  \n",
    "    gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47356ea8",
   "metadata": {
    "papermill": {
     "duration": 0.016047,
     "end_time": "2023-11-09T03:16:27.179270",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.163223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Main\n",
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd41eb95dabbe891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:04.014440Z",
     "start_time": "2023-11-18T04:45:03.999301Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 피쳐 엔지니어링 할 함수에 args가 들어간다면 dependencies에 추가\n",
    "dependencies = {\n",
    "    # \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60b15821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:58:30.576229Z",
     "start_time": "2023-11-18T04:46:09.604461Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026853,
     "end_time": "2023-11-09T03:16:27.296794",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.269941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.51 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_0, Elapsed time: 1.45 seconds, shape((5237980, 7))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 1.45 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 0.06 seconds, shape((5237980, 22))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 1.71 seconds, shape((5237980, 22))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "X_train_numerical shape: (5237980, 19)\n",
      "X_train_categorical shape: (5237980, 1)\n",
      "Y_train shape: (5237980,)\n",
      "Fitting Model - No CV\n",
      "Epoch 1/50\n",
      "2558/2558 [==============================] - 35s 13ms/step - loss: 6.3835 - mean_absolute_error: 6.3835\n",
      "Epoch 2/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3554 - mean_absolute_error: 6.3554\n",
      "Epoch 3/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3522 - mean_absolute_error: 6.3522\n",
      "Epoch 4/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3486 - mean_absolute_error: 6.3486\n",
      "Epoch 5/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3459 - mean_absolute_error: 6.3459\n",
      "Epoch 6/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3436 - mean_absolute_error: 6.3436\n",
      "Epoch 7/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3413 - mean_absolute_error: 6.3413\n",
      "Epoch 8/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3401 - mean_absolute_error: 6.3401\n",
      "Epoch 9/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3381 - mean_absolute_error: 6.3381\n",
      "Epoch 10/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3373 - mean_absolute_error: 6.3373\n",
      "Epoch 11/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3354 - mean_absolute_error: 6.3354\n",
      "Epoch 12/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3340 - mean_absolute_error: 6.3340\n",
      "Epoch 13/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3334 - mean_absolute_error: 6.3334\n",
      "Epoch 14/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3328 - mean_absolute_error: 6.3328\n",
      "Epoch 15/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3321 - mean_absolute_error: 6.3321\n",
      "Epoch 16/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3319 - mean_absolute_error: 6.3319\n",
      "Epoch 17/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3311 - mean_absolute_error: 6.3311\n",
      "Epoch 18/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3300 - mean_absolute_error: 6.3300\n",
      "Epoch 19/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3299 - mean_absolute_error: 6.3299\n",
      "Epoch 20/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3296 - mean_absolute_error: 6.3296\n",
      "Epoch 21/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3287 - mean_absolute_error: 6.3287\n",
      "Epoch 22/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3284 - mean_absolute_error: 6.3284\n",
      "Epoch 23/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3281 - mean_absolute_error: 6.3281\n",
      "Epoch 24/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3272 - mean_absolute_error: 6.3272\n",
      "Epoch 25/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3269 - mean_absolute_error: 6.3269\n",
      "Epoch 26/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3264 - mean_absolute_error: 6.3264\n",
      "Epoch 27/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3263 - mean_absolute_error: 6.3263\n",
      "Epoch 28/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3261 - mean_absolute_error: 6.3261\n",
      "Epoch 29/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3252 - mean_absolute_error: 6.3252\n",
      "Epoch 30/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3252 - mean_absolute_error: 6.3252\n",
      "Epoch 31/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3250 - mean_absolute_error: 6.3250\n",
      "Epoch 32/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3244 - mean_absolute_error: 6.3244\n",
      "Epoch 33/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3243 - mean_absolute_error: 6.3243\n",
      "Epoch 34/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3239 - mean_absolute_error: 6.3239\n",
      "Epoch 35/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3236 - mean_absolute_error: 6.3236\n",
      "Epoch 36/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3233 - mean_absolute_error: 6.3233\n",
      "Epoch 37/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3230 - mean_absolute_error: 6.3230\n",
      "Epoch 38/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3226 - mean_absolute_error: 6.3226\n",
      "Epoch 39/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3229 - mean_absolute_error: 6.3229\n",
      "Epoch 40/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3228 - mean_absolute_error: 6.3228\n",
      "Epoch 41/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3220 - mean_absolute_error: 6.3220\n",
      "Epoch 42/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3216 - mean_absolute_error: 6.3216\n",
      "Epoch 43/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3220 - mean_absolute_error: 6.3220\n",
      "Epoch 44/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3217 - mean_absolute_error: 6.3217\n",
      "Epoch 45/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3212 - mean_absolute_error: 6.3212\n",
      "Epoch 46/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3212 - mean_absolute_error: 6.3212\n",
      "Epoch 47/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3205 - mean_absolute_error: 6.3205\n",
      "Epoch 48/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3209 - mean_absolute_error: 6.3209\n",
      "Epoch 49/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3199 - mean_absolute_error: 6.3199\n",
      "Epoch 50/50\n",
      "2558/2558 [==============================] - 34s 13ms/step - loss: 6.3199 - mean_absolute_error: 6.3199\n",
      "2558/2558 [==============================] - 6s 2ms/step\n",
      "Train NN Score: 6.312043\n"
     ]
    }
   ],
   "source": [
    "if config[\"train_mode\"]:\n",
    "    # 데이터 불러오기\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "\n",
    "    # 데이터 전처리\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # 사용할 피쳐 엔지니어링 함수 선택\n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0'],\n",
    "                                       dependencies=dependencies)\n",
    "    \n",
    "    feature_engineer.generate_global_features(df)\n",
    "    \n",
    "    df = feature_engineer.transform()  # 맨 처음에는 save=True 돌렸으면, 다음부턴 transform(load=True)로 바꾸면된 \n",
    "\n",
    "    df = df.drop(['date_id_copy'], axis=1)\n",
    "\n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "\n",
    "    batch_size = 2048\n",
    "    epochs = 50\n",
    "    hidden_units = [512, 1024, 1024, 512]\n",
    "    dropout_rates = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "    learning_rate = 1e-2\n",
    "    embedding_dims = [25]\n",
    "    \n",
    "    # set scaler\n",
    "    categorical = [\"stock_id\"]\n",
    "    numerical = list(set(df.columns) - set(categorical) - set(['target']))\n",
    "    num_categorical = [len(df[col].unique()) for col in categorical]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    select_columns = list(set(numerical) - set(['dow', 'seconds', 'minute']))\n",
    "    unselect_columns = ['dow', 'seconds', 'minute']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\"scaler\" , scaler, select_columns),\n",
    "            (\"pass\", 'passthrough', unselect_columns)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", preprocessor)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    ckp_path = os.path.join(config['model_dir'], 'my_nn_model_10epoch.h5')\n",
    "    if not os.path.exists(config['model_dir']):\n",
    "        os.mkdir(config['model_dir'])\n",
    "\n",
    "    X_train = df.drop(['target'], axis=1)\n",
    "    Y = df['target']\n",
    "        \n",
    "        \n",
    "    X_tr_continuous = pipe.fit_transform(X_train[numerical])\n",
    "    X_tr_categorical = X_train[categorical].values\n",
    "\n",
    "    print(\"X_train_numerical shape:\",X_tr_continuous.shape)\n",
    "    print(\"X_train_categorical shape:\",X_tr_categorical.shape)\n",
    "    print(\"Y_train shape:\",Y.shape)\n",
    "    \n",
    "    # create model\n",
    "    model = create_mlp(len(numerical), num_categorical, embedding_dims, 1, hidden_units, dropout_rates, learning_rate)\n",
    "    \n",
    "    rlr = ReduceLROnPlateau(monitor='mean_absolute_error', factor=0.1, patience=3, verbose=0, min_delta=1e-4, mode='min')\n",
    "    ckp = ModelCheckpoint(ckp_path, monitor='mean_absolute_error', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    es = EarlyStopping(monitor='mean_absolute_error', min_delta=1e-4, patience=7, mode='min', restore_best_weights=True, verbose=0)\n",
    "\n",
    "    print(f\"Fitting Model - No CV\")\n",
    "    model.fit((X_tr_continuous,X_tr_categorical[:, 0:1]), Y,\n",
    "          epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    model.save_weights(ckp_path)\n",
    "\n",
    "    pred = model.predict((X_tr_continuous,X_tr_categorical[:,0:1]), batch_size=batch_size).ravel()\n",
    "    print(\"Train NN Score:\", mean_absolute_error(Y, pred))\n",
    "\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    rubbish = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbefd84",
   "metadata": {
    "papermill": {
     "duration": 0.014762,
     "end_time": "2023-11-09T03:16:27.326796",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.312034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### upload kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec7130",
   "metadata": {},
   "source": [
    "#### dataset init\n",
    "! /home/username/.local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "#### dataset create \n",
    "! /home/username/.local/bin/kaggle datasets create -p {config['model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a920296",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.933464Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02825,
     "end_time": "2023-11-09T03:16:27.370494",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.342244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ./models/20231127_10:36:02/dataset-metadata.json\n",
      "Starting upload for file my_nn_model_50epoch.h5\n",
      "100%|██████████████████████████████████████| 8.22M/8.22M [00:02<00:00, 3.14MB/s]\n",
      "Upload successful: my_nn_model_50epoch.h5 (8MB)\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/jhk3211/model-nn-version-yongmin-0\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462f4d1ce1d4ee13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936242Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    # \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90e686-4185-475d-b370-60228956165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"infer_mode\"]:\n",
    "\n",
    "    model_list = []\n",
    "    print(\"Loading Models...\")\n",
    "    final_model = create_mlp(len(numerical), num_categorical, embedding_dims, 1, hidden_units, dropout_rates, learning_rate)\n",
    "\n",
    "    model_path = '/kaggle/input/optiv-try-an-simple-neural-network-with-keras/NN_Models/my_nn_model_10epoch.h5'\n",
    "    final_model.load_weights(model_path)\n",
    "    model_list.append(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2dd30",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936428Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-10T07:56:37.614577Z",
     "iopub.status.busy": "2023-11-10T07:56:37.613731Z",
     "iopub.status.idle": "2023-11-10T07:56:54.449199Z",
     "shell.execute_reply": "2023-11-10T07:56:54.448361Z",
     "shell.execute_reply.started": "2023-11-10T07:56:37.614538Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 53.62895,
     "end_time": "2023-11-09T03:17:21.014741",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.385791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config[\"infer_mode\"]:\n",
    "    import optiver2023\n",
    "\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    y_min, y_max = -64, 64\n",
    "    qps = []\n",
    "    counter = 0\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # set scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    select_columns = list(set(numerical) - set(['dow', 'seconds', 'minute']))\n",
    "    unselect_columns = ['dow', 'seconds', 'minute']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\"scaler\" , scaler, select_columns),\n",
    "            (\"pass\", 'passthrough', unselect_columns)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", preprocessor)\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "    \n",
    "    feature_engineer = FeatureEngineer(df)\n",
    "    feature_engineer.generate_global_features(df)\n",
    "\n",
    "    df = feature_engineer.transform()\n",
    "\n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "\n",
    "    categorical = [\"stock_id\"]\n",
    "    numerical = list(set(df.columns) - set(categorical) - set(['target']))\n",
    "    num_categorical = [len(df[col].unique()) for col in categorical]\n",
    "\n",
    "    pipe.fit(df[numeric])\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        \n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "    \n",
    "        if counter > 0:\n",
    "            cache_df = cache.groupby(['stock_id']).tail(21).sort_values(\n",
    "                                     by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        \n",
    "        # feature engineering\n",
    "        feature_engineer = FeatureEngineer(cache_df, infer=True, feature_versions=['feature_version_yongmin_0'],\n",
    "                                           dependencies=dependencies)\n",
    "        \n",
    "        cache_df = feature_engineer.transform()\n",
    "        \n",
    "        feat = cache_df[-len(test):]\n",
    "\n",
    "        X_num = pipe.transform(feat[numerical])\n",
    "        X_cat = feat[categorical].values\n",
    "        \n",
    "        print(\"X_train_numerical shape:\",X_num.shape)\n",
    "        print(\"X_train_categorical shape:\",X_cat.shape)\n",
    "        \n",
    "        # feat = generate_all_features(cache)[-len(test):]\n",
    "        test_predss = np.zeros(feat.shape[0])\n",
    "\n",
    "        \n",
    "        # prediction\n",
    "        for i in range(config[\"inference_n_splits\"]):\n",
    "            inference_prediction = final_model.predict((X_num, X_cat[:,0:1], X_cat[:,1:2]))\n",
    "            \n",
    "            test_predss += inference_prediction / config[\"inference_n_splits\"]\n",
    "            \n",
    "        test_predss = zero_sum(test_predss, test['bid_size'] + test['ask_size'])\n",
    "        \n",
    "        clipped_predictions = np.clip(test_predss, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        \n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        \n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa41bbe-e7c9-43d4-aff0-b67a80c58a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec361d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-18T04:45:49.936534Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-10T07:56:55.125299Z",
     "iopub.status.idle": "2023-11-10T07:56:55.125615Z",
     "shell.execute_reply": "2023-11-10T07:56:55.125474Z",
     "shell.execute_reply.started": "2023-11-10T07:56:55.125459Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.042958,
     "end_time": "2023-11-09T03:17:21.150593",
     "exception": false,
     "start_time": "2023-11-09T03:17:21.107635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single 1fold final / fianl no\n",
    "# single 1fold final / fianl\n",
    "# single 5fold final / fianl no\n",
    "# single 5fold final / fianl\n",
    "# stacking 1fold final / fianl no\n",
    "# stacking 1fold final / fianl\n",
    "# stacking 5fold final / fianl no\n",
    "# stacking 5fold final / fianl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yongmin-venv",
   "language": "python",
   "name": "yongmin-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 99.440385,
   "end_time": "2023-11-09T03:17:22.555761",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-09T03:15:43.115376",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
