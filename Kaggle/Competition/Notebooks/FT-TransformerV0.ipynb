{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f015b082-2a5b-4260-833b-430d7fa504fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 12:11:51.891322: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-07 12:11:51.910854: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 12:11:51.910873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 12:11:51.910885: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 12:11:51.914962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/yongmin/yongmin-venv/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from numba import njit, prange\n",
    "from itertools import combinations\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import gelu, softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "import random\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, ReLU, BatchNormalization, Activation, Concatenate\n",
    "from copy import deepcopy\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "# from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import joblib\n",
    "import functools\n",
    "\n",
    "import gc\n",
    "gc.collect();\n",
    "\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47af01c1-9fc2-4dc2-bb2b-609b2f4f4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference, both\n",
    "KAGGLE_DATASET_NAME = \"fttransformer-version-yongmin-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2da88e-bbfc-4e61-99c6-4f8c7fa5a716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4879e6-19c4-49ec-a37a-9845a8502eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"final_mode\": False,  # True : using final model, False : not using final model\n",
    "    \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "    'target': 'target',\n",
    "\n",
    "    'split_method': 'rolling',  # time_series, rolling, blocking, holdout\n",
    "    'n_splits': 3,  # number of splits\n",
    "    'correct': True,  # correct boundary\n",
    "    'gap': 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "\n",
    "    'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "    'train_test_ratio': 0.9,  # train, test ratio\n",
    "\n",
    "    'optuna_random_state': 42,\n",
    "}\n",
    "config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "    \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "    \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcec275-4f60-472a-bc1a-98d9ba4de03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      "100%|████████████████████████████████████████| 201M/201M [00:06<00:00, 33.2MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:06<00:00, 32.0MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dca422-8cf3-4cdb-9ab4-d36bdf36035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470390be-3975-443f-a1dd-7b7a257fc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3b04b7-5573-4298-96ae-4099784d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(self, data, infer=False):\n",
    "        self.data = memory_reduction.reduce_mem_usage(data)\n",
    "        self.infer = infer\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def handle_missing_data(self):\n",
    "        # 결측치 처리 코드\n",
    "        self.data = self.data.dropna(subset=[\"target\"]) if self.infer == False else self.data\n",
    "        self.data = self.data.fillna(0, columns = ['far_price', 'near_price'])\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def handle_outliers(self):\n",
    "        # 이상치 처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def normalize(self):\n",
    "        # 정규화 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape})\")\n",
    "    def custom_preprocessing(self):\n",
    "        # 사용자 정의 전처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func.__name__} : shape({self.data.shape}) \\n\\n {self.data}\")\n",
    "    def transform(self):\n",
    "        # 전처리 수행 코드 (위의 메소드 활용 가능)\n",
    "        self.handle_missing_data()\n",
    "        # self.handle_outliers()\n",
    "        # self.normalize()\n",
    "        # self.custom_preprocessing()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ff1fdf-7a5c-456f-bdfe-01b8594dc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084010b1-c76a-4b3a-b4de-a64406cfd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window_size=7):\n",
    "    price_diff = data['wap'].diff()\n",
    "    gain = price_diff.where(price_diff > 0, 0)\n",
    "    loss = -price_diff.where(price_diff < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window_size).mean()\n",
    "    avg_loss = loss.rolling(window=window_size).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6749ac6-5358-463f-857e-d769f33eddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e20cf08-4342-4c32-8ad1-af0cfd412a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_0(self, *args, version_name=\"feature_version_yongmin_0\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df['dow'] = self.data[\"date_id\"] % 5\n",
    "        df['seconds'] = self.data['seconds_in_bucket'] % 60\n",
    "        df['minute'] = self.data['seconds_in_bucket'] // 60\n",
    "    \n",
    "        df[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "        df['cum_wap'] = self.data.groupby(['stock_id'])['wap'].cumprod()\n",
    "    \n",
    "        for i in [1, 6]: # 1, 6도 생각해보기\n",
    "            df[f'pct_change_{i}'] = self.data.groupby(['stock_id', 'seconds_in_bucket'])['wap'].pct_change(i).fillna(0)\n",
    "\n",
    "            f = lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "            df[f'polarize_pct_{i}'] = df[f'pct_change_{i}'].apply(f)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_yongmin_1(self, *args, version_name=\"feature_version_yongmin_1\"):\n",
    "        \n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        \n",
    "        window_size = 6\n",
    "        short_window = 1\n",
    "        long_window = 6\n",
    "\n",
    "        df['vol_st'] = self.data.groupby(['stock_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['rolling_vol_di'] = self.data.groupby(['date_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "        df['std_st'] = self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['wap_pctch'] = self.data.groupby(['stock_id','date_id'])['wap'].pct_change().values*100\n",
    "        df['short_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=short_window, adjust=False).mean().values\n",
    "        df['long_ema'] = self.data.groupby(['stock_id'])['wap'].ewm(span=long_window, adjust=False).mean().values\n",
    "        wap_mean = self.data['wap'].mean()\n",
    "        df['wap_vs_market'] = self.data['wap'] - self.data.groupby(['stock_id'])['wap'].transform('mean')\n",
    "        df['macd'] = df['short_ema'] - df['long_ema']\n",
    "        \n",
    "        # Bollinger Bands calculation within each stock, date, and time\n",
    "        df['bollinger_upper'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values + 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        df['bollinger_lower'] = self.data.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values - 2 * self.data.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "        # RSI calculation within each stock, date, and time\n",
    "        df['rsi'] = self.data.groupby(['stock_id']).apply(calculate_rsi).values\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_1(self, *args, version_name=\"feature_version_alvin_1\"):\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "        df[\"mid_price\"] = self.data.eval(\"(ask_price + bid_price) / 2\")\n",
    "        df[\"liquidity_imbalance\"] = self.data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n",
    "        df[\"matched_imbalance\"] = self.data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n",
    "        df[\"size_imbalance\"] = self.data.eval(f\"bid_size / ask_size+{EPS}\")\n",
    "\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = self.data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n",
    "\n",
    "        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "            triplet_feature = calculate_triplet_imbalance_numba(c, self.data)\n",
    "            df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "        return df\n",
    "\n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        final_data = concat_df\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aea62f3-761a-44a4-95d7-f5a4709e8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class QuantileTF:\n",
    "    def __init__(self):\n",
    "        self.scaler = QuantileTransformer(output_distribution='normal').set_output(transform=\"pandas\")\n",
    "        self.pipe = None\n",
    "\n",
    "    def initialize_pipeline(self, df, usecols, passcols):\n",
    "        columnTF = ColumnTransformer(\n",
    "                                        transformers = [\n",
    "                                            (\"scaler\" , self.scaler, usecols),\n",
    "                                            (\"pass\", 'passthrough', passcols)\n",
    "                                        ]\n",
    "                                    )\n",
    "\n",
    "        self.pipe = Pipeline([\n",
    "                                (\"scaler\", columnTF.set_output(transform=\"pandas\"))\n",
    "                            ])\n",
    "\n",
    "    def fit(self, df, usecols, passcols):\n",
    "        self.initialize_pipeline(df, usecols, passcols)\n",
    "        self.pipe.fit(df)\n",
    "        \n",
    "    def transform(self, df):\n",
    "        cols = [col[8:] for col in self.pipe.get_feature_names_out() if \"scaler\" in col]\n",
    "        return pd.DataFrame(self.pipe.transform(df), columns = cols)\n",
    "\n",
    "    def fit_transform(self, df, usecols, passcols):\n",
    "        self.initialize_pipeline(df, usecols, passcols)\n",
    "\n",
    "        \n",
    "        return self.pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "681c11f6-7437-431e-b223-8cfbe7c2e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset_(\n",
    "    dataframe: pd.DataFrame,\n",
    "    target: str = None,\n",
    "    shuffle: bool = True,\n",
    "    batch_size: int = 512,\n",
    "):\n",
    "    df = dataframe.copy()\n",
    "    if target:\n",
    "        labels = df.pop(target)\n",
    "        # dataset = {}\n",
    "        # for key, value in df.items():\n",
    "        #     #dataset[key] = value[:, tf.newaxis] # old version\n",
    "        #     dataset[key] = np.array(value)[:, tf.newaxis] # modified\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df.to_numpy().astype(np.float32), labels))\n",
    "    else:\n",
    "        # dataset = {}\n",
    "        # for key, value in df.items():\n",
    "        #     #dataset[key] = value[:, tf.newaxis] # old version\n",
    "        #     dataset[key] = np.array(value)[:, tf.newaxis] # modified\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(df.to_numpy().astype(np.float32))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabd8a3-4470-45bb-a433-490f5aafa2ae",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58330c3c-16a6-4e0f-acfb-7001ceb3a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # parametreleri\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # batch-layer\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class FTTransformer(keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "            categories,\n",
    "            num_continuous,\n",
    "            dim,\n",
    "            dim_out,\n",
    "            depth,\n",
    "            embedding_dim,\n",
    "            heads,\n",
    "            attn_dropout,\n",
    "            ff_dropout,\n",
    "            mlp_hidden,\n",
    "            normalize_continuous = True):\n",
    "\n",
    "        super(FTTransformer, self).__init__()\n",
    "\n",
    "        # --> continuous inputs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.normalize_continuous = normalize_continuous\n",
    "        if normalize_continuous:\n",
    "            self.continuous_normalization = layers.LayerNormalization()\n",
    "\n",
    "        # --> categorical inputs\n",
    "\n",
    "        # embedding\n",
    "        self.embedding_layers = []\n",
    "        for number_of_classes in categories:\n",
    "            self.embedding_layers.append(layers.Embedding(input_dim = number_of_classes, output_dim = dim))\n",
    "        self.embb = layers.Embedding(input_dim = 1000, output_dim = dim)\n",
    "        self.flatten_output = layers.Flatten()\n",
    "\n",
    "        # concatenation\n",
    "        self.embedded_concatenation = layers.Concatenate(axis=1)\n",
    "        self.cont_embedded_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # adding transformers\n",
    "        self.transformers = []\n",
    "        for _ in range(depth):\n",
    "            self.transformers.append(TransformerBlock(dim, heads, dim))\n",
    "        self.flatten_transformer_output = layers.Flatten()\n",
    "\n",
    "        # --> MLP\n",
    "        self.pre_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # mlp layers\n",
    "        self.mlp_layers = []\n",
    "        for size, activation in mlp_hidden:\n",
    "            self.mlp_layers.append(layers.Dense(size, activation=activation))\n",
    "\n",
    "        self.output_layer = layers.Dense(dim_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        continuous_inputs  = inputs[0]\n",
    "        categorical_inputs = inputs[1:]\n",
    "\n",
    "        # --> continuous\n",
    "        if self.normalize_continuous:\n",
    "            continuous_inputs = self.continuous_normalization(continuous_inputs)\n",
    "\n",
    "        continuous_inputs = self.embb(continuous_inputs)\n",
    "\n",
    "        # --> categorical\n",
    "        embedding_outputs = []\n",
    "        for categorical_input, embedding_layer in zip(categorical_inputs, self.embedding_layers):\n",
    "            embedding_outputs.append(embedding_layer(categorical_input))\n",
    "        categorical_inputs = self.embedded_concatenation(embedding_outputs)\n",
    "    \n",
    "        trans_input = self.pre_concatenation([continuous_inputs, categorical_inputs])\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            trans_input = transformer(trans_input)\n",
    "        mlp_input = self.flatten_transformer_output(trans_input)\n",
    "        # --> MLP\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            mlp_input = mlp_layer(mlp_input)\n",
    "        return self.output_layer(mlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46025a59-042d-4fe0-900a-43633e544812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch_data(x, y, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset_batch = dataset.batch(batch_size, drop_remainder = True)\n",
    "    return dataset_batch\n",
    "\n",
    "def get_X_from_groups(feature_set, groups):\n",
    "    result = []\n",
    "    for group in groups:\n",
    "        result.append(feature_set[group])\n",
    "    return result\n",
    "\n",
    "def get_X_from_features(feature_set, cont_features, cat_features):\n",
    "    groups = [cont_features]\n",
    "    groups.extend(cat_features)\n",
    "    return get_X_from_groups(feature_set, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4924ebbc-f7f3-4c9b-b724-a6d8fd08397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda8a4dd-e3ec-4a5c-95f7-e628b4351153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.64 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_0, Elapsed time: 2.43 seconds, shape((5237980, 9))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_yongmin_1, Elapsed time: 11.11 seconds, shape((5237980, 11))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_alvin_1, Elapsed time: 1.62 seconds, shape((5237980, 27))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 15.17 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 0.44 seconds, shape((5237980, 62))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 16.56 seconds, shape((5237980, 63))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config[\"train_mode\"]:\n",
    "    # Read Data\n",
    "    df = (pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "          .drop(['time_id', 'row_id'], axis=1))\n",
    "\n",
    "    # Feature Engineering\n",
    "\n",
    "    feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1', 'feature_version_alvin_1'],\n",
    "                                       dependencies=dependencies)\n",
    "    \n",
    "    feature_engineer.generate_global_features(df)\n",
    "    \n",
    "    df = feature_engineer.transform()\n",
    "    \n",
    "    # fillna\n",
    "    df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(method='ffill')\n",
    "          .fillna(0)\n",
    "         )\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # Set Variable Type\n",
    "    categorical = ['stock_id', 'date_id', 'imbalance_buy_sell_flag', 'polarize_pct_1', 'polarize_pct_6']\n",
    "    continuous = list(set(df.columns) - set(categorical) - set(['target', 'date_id_copy']))\n",
    "    features = categorical + continuous\n",
    "\n",
    "    cats = [df[col].unique().shape[0] for col in categorical]\n",
    "\n",
    "    scaler = QuantileTransformer(output_distribution = 'normal')\n",
    "    \n",
    "    rubbish = gc.collect()\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    LR = 0.001\n",
    "    EPOCH = 10\n",
    "    MODEL_NAME = 'fttransformer_10epochV0.h5'\n",
    "\n",
    "    ckp_path = os.path.join(config['model_dir'], MODEL_NAME)\n",
    "    if not os.path.exists(config['model_dir']):\n",
    "        os.mkdir(config['model_dir'])\n",
    "\n",
    "    rlr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=3, verbose=0, min_delta=1e-5, mode='min')\n",
    "    ckp = ModelCheckpoint(ckp_path, monitor='val_mae', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    es = EarlyStopping(monitor='val_mae', min_delta=1e-4, patience=5, mode='min', restore_best_weights=True, verbose=0)\n",
    "\n",
    "    model_callbacks = [rlr, ckp, es]\n",
    "    rubbish = gc.collect()\n",
    "\n",
    "    SPLIT_DAY = 390\n",
    "    \n",
    "    split = df['date_id'] > SPLIT_DAY\n",
    "    df_train = df[~split]\n",
    "    df_valid = df[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "334c5d10-f8df-4714-bff2-af4e9c390356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4247980, 62) (4247980,) (990000, 62) (990000,)\n",
      "56 Continuous Features Scaling Done\n",
      "Dataset Transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 12:13:12.102638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.105343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.105413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.105983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.106035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.106069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.172983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.173054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.173106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 12:13:12.173148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5347 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training - HoldOut CV\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ft_transformer/layer_normalization/gamma:0', 'ft_transformer/layer_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ft_transformer/layer_normalization/gamma:0', 'ft_transformer/layer_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ft_transformer/layer_normalization/gamma:0', 'ft_transformer/layer_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ft_transformer/layer_normalization/gamma:0', 'ft_transformer/layer_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 12:13:15.966798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-12-07 12:13:16.622145: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2218accaf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-07 12:13:16.622163: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-12-07 12:13:16.625882: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-07 12:13:16.670725: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47323/66375 [====================>.........] - ETA: 11:23 - loss: 6.5026 - mae: 6.5026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66375/66375 [==============================] - 2620s 39ms/step - loss: 6.4977 - mae: 6.4977 - val_loss: 6.0216 - val_mae: 6.0216 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66375/66375 [==============================] - 1724s 26ms/step - loss: 6.4975 - mae: 6.4975 - val_loss: 6.0215 - val_mae: 6.0215 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66375/66375 [==============================] - 1345s 20ms/step - loss: 6.4975 - mae: 6.4975 - val_loss: 6.0220 - val_mae: 6.0220 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66375/66375 [==============================] - 1430s 22ms/step - loss: 6.4975 - mae: 6.4975 - val_loss: 6.0215 - val_mae: 6.0215 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "13817/66375 [=====>........................] - ETA: 15:12 - loss: 6.4932 - mae: 6.4932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect();\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Training - HoldOut CV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39msave(ckp_path)\n\u001b[1;32m     52\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/yongmin-venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if config['train_mode']:\n",
    "    # holdout\n",
    "    X_train = df_train.drop(['target'], axis=1)\n",
    "    y_train = df_train['target']\n",
    "\n",
    "    X_test = df_valid.drop(['target'], axis=1)\n",
    "    y_test = df_valid['target']\n",
    "    \n",
    "    if 'date_id_copy' in X_train.columns:\n",
    "        X_train.drop(['date_id_copy'], axis=1)\n",
    "        X_test.drop(['date_id_copy'], axis=1)\n",
    "            \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    \n",
    "    X_train[continuous] = scaler.fit_transform(X_train[continuous])\n",
    "    X_test[continuous] = scaler.transform(X_test[continuous])\n",
    "    print(f\"{len(continuous)} Continuous Features Scaling Done\")\n",
    "\n",
    "    X_train = get_X_from_features(X_train, continuous, categorical)\n",
    "    X_test = get_X_from_features(X_test, continuous, categorical)\n",
    "    print(\"Dataset Transformed\")\n",
    "\n",
    "    model = FTTransformer(\n",
    "                        categories = cats, # number of unique elements in each categorical feature\n",
    "                        num_continuous = len(continuous),    # number of numerical features\n",
    "                        dim = 16,                # embedding/transformer dimension\n",
    "                        dim_out = 1,             # dimension of the model output\n",
    "                        depth = 4,  \n",
    "                        embedding_dim=32,       # number of transformer layers in the stack\n",
    "                        heads = 6,               # number of attention heads\n",
    "                        attn_dropout = 0.5,      # attention layer dropout in transformers\n",
    "                        ff_dropout = 0.5,        # feed-forward layer dropout in transformers\n",
    "                        mlp_hidden = [(512, 'relu'), (128, 'relu'), (64, 'relu'), (32, 'relu')]\n",
    "                        #mlp_hidden = [(1024, 'relu'), (256, 'relu'), (128, 'relu'), (64, 'relu'), (32, 'relu')] # mlp layer dimensions and activations\n",
    "                    )\n",
    "\n",
    "    model.compile(loss = 'mae', \n",
    "                  metrics=['mae'], \n",
    "                  optimizer = tf.keras.optimizers.AdamW(LR))\n",
    "\n",
    "    gc.collect();\n",
    "\n",
    "    print(\"Model Training - HoldOut CV\")\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data = (X_test, y_test),\n",
    "              epochs=EPOCH, \n",
    "              batch_size=BATCH_SIZE,\n",
    "              callbacks=model_callbacks)\n",
    "        \n",
    "    model.save(ckp_path)\n",
    "    \n",
    "    K.clear_session()\n",
    "    del model\n",
    "    rubbish = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdb111-d33d-43c4-8f70-dc5687513d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96244c8a-598d-4437-9c45-18f8abde2023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T16:06:25.374325Z",
     "iopub.status.busy": "2023-12-05T16:06:25.374180Z",
     "iopub.status.idle": "2023-12-05T16:06:25.376183Z",
     "shell.execute_reply": "2023-12-05T16:06:25.375983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = (pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "#       .drop(['time_id', 'row_id'], axis=1))\n",
    "\n",
    "# # Feature Engineering\n",
    "\n",
    "# feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_yongmin_0', 'feature_version_yongmin_1'],\n",
    "#                                    dependencies=dependencies)\n",
    "\n",
    "# feature_engineer.generate_global_features(df)\n",
    "\n",
    "# df = feature_engineer.transform()\n",
    "\n",
    "# # fillna\n",
    "# df = (df.replace([np.inf, -np.inf], np.nan)\n",
    "#       .fillna(method='ffill')\n",
    "#       .fillna(0)\n",
    "#      )\n",
    "\n",
    "# df = reduce_mem_usage(df)\n",
    "# rubbish = gc.collect()\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "# LR = 0.001\n",
    "# EPOCH = 30\n",
    "# INPUT_SHAPE = df.shape[-1] - 2 # -2 for target and date_id_copy\n",
    "\n",
    "# final_model = Sequential([\n",
    "#                     Input(shape = (INPUT_SHAPE), batch_size=BATCH_SIZE),\n",
    "#                     SAINT(3, 6, 8, 4, 4),\n",
    "#                     Dense(1 , activation = 'linear')\n",
    "# ])\n",
    "\n",
    "# final_model.compile(tf.keras.optimizers.Adam(LR), 'mae', metrics=['mae'])\n",
    "\n",
    "# model_path = \"./models/20231130_11:05:26 /saint_30epoch.h5\"\n",
    "# final_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62af2d-e7c3-4a63-ac2d-4619a9f89af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yongmin-venv",
   "language": "python",
   "name": "yongmin-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
